{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7a746a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5bfd13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/venv/env1/lib/python3.11/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████████████| 3/3 [00:04<00:00,  1.54s/it]\n",
      "Some weights of LlavaForConditionalGenerationScal were not initialized from the model checkpoint at llava-hf/llava-1.5-7b-hf and are newly initialized: ['language_model.model.layers.0.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.1.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.10.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.11.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.12.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.13.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.14.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.15.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.16.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.17.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.18.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.19.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.2.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.20.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.21.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.22.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.23.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.24.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.25.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.26.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.27.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.28.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.29.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.3.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.30.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.31.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.4.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.5.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.6.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.7.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.8.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.9.self_attn.rotary_emb.inv_freq']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Controlled_Images_A llava1.5\n",
      "  0%|                                                   | 0/330 [00:00<?, ?it/s]Prompt: <image>\n",
      "USER: Where is the beer bottle in relation to the armchair? Answer with left, right, on or under.\n",
      "ASSISTANT:\n",
      "Generation: \n",
      "Golden: On\n",
      "0 1 0.0\n",
      "  0%|                                         | 1/330 [00:12<1:07:47, 12.36s/it]Prompt: <image>\n",
      "USER: Where is the scarf in relation to the armchair? Answer with left, right, on or under.\n",
      "ASSISTANT:\n",
      "Generation: Right\n",
      "Golden: Right\n",
      "1 2 0.5\n",
      "  1%|▎                                          | 2/330 [00:15<39:00,  7.14s/it]Prompt: <image>\n",
      "USER: Where is the orange in relation to the armchair? Answer with left, right, on or under.\n",
      "ASSISTANT:\n",
      "Generation: Right\n",
      "Golden: Right\n",
      "2 3 0.6666666666666666\n",
      "  1%|▍                                          | 3/330 [00:19<29:26,  5.40s/it]Prompt: <image>\n",
      "USER: Where is the scarf in relation to the table? Answer with left, right, on or under.\n",
      "ASSISTANT:\n",
      "Generation: \n",
      "Golden: Under\n",
      "2 4 0.5\n",
      "  1%|▌                                          | 4/330 [00:30<42:30,  7.82s/it]Prompt: <image>\n",
      "USER: Where are the sunglasses in relation to the chair? Answer with left, right, on or under.\n",
      "ASSISTANT:\n",
      "Generation: Left\n",
      "Golden: Left\n",
      "3 5 0.6\n",
      "  2%|▋                                          | 5/330 [00:34<33:38,  6.21s/it]Prompt: <image>\n",
      "USER: Where is the toilet roll in relation to the table? Answer with left, right, on or under.\n",
      "ASSISTANT:\n",
      "Generation: Right\n",
      "Golden: Left\n",
      "3 6 0.5\n",
      "  2%|▊                                          | 6/330 [00:37<28:10,  5.22s/it]Prompt: <image>\n",
      "USER: Where is the bottle in relation to the chair? Answer with left, right, on or under.\n",
      "ASSISTANT:\n",
      "Generation: Right\n",
      "Golden: Right\n",
      "4 7 0.5714285714285714\n",
      "  2%|▉                                          | 7/330 [00:40<24:56,  4.63s/it]^C\n",
      "Process Process-13:\n",
      "Process Process-15:\n",
      "Process Process-16:\n",
      "Process Process-12:\n",
      "Process Process-8:\n",
      "Process Process-11:\n",
      "Process Process-9:\n",
      "Process Process-10:\n",
      "Process Process-14:\n",
      "Process Process-7:\n",
      "Process Process-6:\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x765d7ff92840>\n",
      "Traceback (most recent call last):\n",
      "  File \"/venv/env1/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 1477, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/venv/env1/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 1441, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/venv/env1/lib/python3.11/multiprocessing/process.py\", line 149, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/venv/env1/lib/python3.11/multiprocessing/popen_fork.py\", line 40, in wait\n",
      "    if not wait([self.sentinel], timeout):\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/venv/env1/lib/python3.11/multiprocessing/connection.py\", line 948, in wait\n",
      "    ready = selector.select(timeout)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/venv/env1/lib/python3.11/selectors.py\", line 415, in select\n",
      "    fd_event_list = self._selector.poll(timeout)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt: \n",
      "  2%|▉                                          | 7/330 [00:44<34:22,  6.38s/it]\n",
      "Traceback (most recent call last):\n",
      "  File \"/workspace/AdaptVis/main_aro.py\", line 93, in <module>\n",
      "    main(args)\n",
      "  File \"/workspace/AdaptVis/main_aro.py\", line 78, in main\n",
      "    scores, correct_id = model.get_out_scores_wh_batched(args.dataset,joint_loader,args.method,args.weight,args.option,args.threshold,args.weight1,args.weight2)\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/venv/env1/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspace/AdaptVis/model_zoo/llava15.py\", line 461, in get_out_scores_wh_batched\n",
      "    result = self.adaptvis_bidirectional_reasoning(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/venv/env1/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspace/AdaptVis/model_zoo/llava15.py\", line 899, in adaptvis_bidirectional_reasoning\n",
      "    output_reverse, results_reverse = self._generate(image, query_reverse, weight=selected_weight_reverse)\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspace/AdaptVis/model_zoo/llava15.py\", line 791, in _generate\n",
      "    output = self.model.generate(\n",
      "             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/venv/env1/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/venv/env1/lib/python3.11/site-packages/transformers/generation/utils.py\", line 1527, in generate\n",
      "    result = self._greedy_search(\n",
      "             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspace/AdaptVis/model_zoo/llava15.py\", line 139, in _add_weight_greedy_search\n",
      "    outputs = self(\n",
      "              ^^^^^\n",
      "  File \"/venv/env1/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/venv/env1/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspace/AdaptVis/model_zoo/llava/modeling_llava_scal.py\", line 458, in forward\n",
      "    outputs = self.language_model(\n",
      "              ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/venv/env1/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/venv/env1/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspace/AdaptVis/model_zoo/llama/modeling_llama_add_attn.py\", line 872, in forward\n",
      "    outputs = self.model(\n",
      "              ^^^^^^^^^^^\n",
      "  File \"/venv/env1/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/venv/env1/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspace/AdaptVis/model_zoo/llama/modeling_llama_add_attn.py\", line 708, in forward\n",
      "    layer_outputs = decoder_layer(\n",
      "                    ^^^^^^^^^^^^^^\n",
      "  File \"/venv/env1/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/venv/env1/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspace/AdaptVis/model_zoo/llama/modeling_llama_add_attn.py\", line 389, in forward\n",
      "    hidden_states, self_attn_weights, present_key_value = self.self_attn(\n",
      "                                                          ^^^^^^^^^^^^^^^\n",
      "  File \"/venv/env1/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/venv/env1/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspace/AdaptVis/model_zoo/llama/modeling_llama_add_attn.py\", line 312, in forward\n",
      "    np.save(f\"{save_path}diff_{idx}_start{start_idx}_end{end_idx}.npy\", ori.cpu().detach().numpy())\n",
      "  File \"/venv/env1/lib/python3.11/site-packages/numpy/lib/npyio.py\", line 546, in save\n",
      "    format.write_array(fid, arr, allow_pickle=allow_pickle,\n",
      "  File \"/venv/env1/lib/python3.11/site-packages/numpy/lib/format.py\", line 730, in write_array\n",
      "    array.tofile(fp)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!bash my_run.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd62f9b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/env1/lib/python3.11/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab2304a29726495cae4fd663a28b2174",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlavaForConditionalGenerationScal were not initialized from the model checkpoint at llava-hf/llava-1.5-7b-hf and are newly initialized: ['language_model.model.layers.0.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.1.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.10.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.11.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.12.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.13.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.14.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.15.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.16.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.17.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.18.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.19.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.2.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.20.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.21.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.22.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.23.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.24.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.25.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.26.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.27.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.28.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.29.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.3.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.30.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.31.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.4.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.5.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.6.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.7.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.8.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.9.self_attn.rotary_emb.inv_freq']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "# from transformers import LlavaNextProcessor, LlavaNextForConditionalGeneration\n",
    "import random\n",
    "from transformers import AutoProcessor, LlamaTokenizerFast, CLIPImageProcessor\n",
    "import pdb\n",
    "# import probe_llava\n",
    "from model_zoo.llava import  LlavaForConditionalGeneration, LlavaForConditionalGenerationScal\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "from collections import Counter\n",
    "# from model_zoo.utils import normalize_answer,chat_completion_request,run_conversation\n",
    "\n",
    "from PIL import Image\n",
    "import math\n",
    "MODEL='llava-hf/llava-1.5-7b-hf'\n",
    "root_dir='data'\n",
    "device = 'cuda'\n",
    "model = LlavaForConditionalGenerationScal.from_pretrained(MODEL, revision='a272c74',cache_dir=root_dir,ignore_mismatched_sizes=True).eval().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "188a76fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = CLIPImageProcessor.from_pretrained(MODEL, revision='a272c74',cache_dir=root_dir)\n",
    "tokenizer = LlamaTokenizerFast.from_pretrained(MODEL, revision='a272c74',cache_dir=root_dir)\n",
    "processor = AutoProcessor.from_pretrained(MODEL, revision='a272c74',cache_dir=root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c3c137b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "# from transformers import LlavaNextProcessor, LlavaNextForConditionalGeneration\n",
    "import random\n",
    "from transformers import AutoProcessor, LlamaTokenizerFast, CLIPImageProcessor\n",
    "import pdb\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "from collections import Counter\n",
    "# from model_zoo.utils import normalize_answer,chat_completion_request,run_conversation\n",
    "\n",
    "from PIL import Image\n",
    "import math\n",
    "MODEL='llava-hf/llava-1.5-7b-hf'\n",
    "\n",
    "import copy\n",
    "import inspect\n",
    "import warnings\n",
    "from dataclasses import dataclass\n",
    "from typing import TYPE_CHECKING, Any, Callable, Dict, List, Optional, Tuple, Union\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torch import nn\n",
    "\n",
    "from transformers.generation.logits_process import (\n",
    "    LogitsProcessorList,\n",
    ")\n",
    "from transformers.generation.stopping_criteria import (\n",
    "    StoppingCriteria,\n",
    "    StoppingCriteriaList,\n",
    "    validate_stopping_criteria,\n",
    ")\n",
    "import transformers\n",
    "from transformers.generation.utils import SampleOutput, SampleDecoderOnlyOutput, SampleEncoderDecoderOutput,GenerateEncoderDecoderOutput,GenerateDecoderOnlyOutput,GenerateNonBeamOutput\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "def _add_weight_greedy_search(\n",
    "    self,\n",
    "    input_ids: torch. LongTensor,\n",
    "    logits_processor: Optional[LogitsProcessorList] = None,\n",
    "    stopping_criteria: Optional[StoppingCriteriaList] = None,\n",
    "    max_length: Optional[int] = None,\n",
    "    pad_token_id: Optional[int] = None,\n",
    "    eos_token_id: Optional[Union[int, List[int]]] = None,\n",
    "    output_attentions: Optional[bool] = None,\n",
    "    output_hidden_states: Optional[bool] = None,\n",
    "    output_scores: Optional[bool] = None,\n",
    "    output_logits: Optional[bool] = None,\n",
    "    return_dict_in_generate: Optional[bool] = None,\n",
    "    synced_gpus: bool = False,\n",
    "    # keys:Optional[torch.Tensor] = None,\n",
    "    weight: Optional[float] = None,\n",
    "    adjust_method: Optional[str] = None,\n",
    "    pos: Optional[torch.Tensor] = None,\n",
    "    streamer: Optional[\"BaseStreamer\"] = None,\n",
    "    **model_kwargs,\n",
    "    ) -> Union[GenerateNonBeamOutput, torch.LongTensor]:\n",
    "    # init values\n",
    "    logits_processor = logits_processor if logits_processor is not None else LogitsProcessorList()\n",
    "    stopping_criteria = stopping_criteria if stopping_criteria is not None else StoppingCriteriaList()\n",
    "    if max_length is not None:\n",
    "        warnings.warn(\n",
    "            \"`max_length` is deprecated in this function, use\"\n",
    "            \" `stopping_criteria=StoppingCriteriaList([MaxLengthCriteria(max_length=max_length)])` instead.\",\n",
    "            UserWarning,\n",
    "        )\n",
    "        stopping_criteria = validate_stopping_criteria(stopping_criteria, max_length)\n",
    "    pad_token_id = pad_token_id if pad_token_id is not None else self.generation_config.pad_token_id\n",
    "    eos_token_id = eos_token_id if eos_token_id is not None else self.generation_config.eos_token_id\n",
    "    if isinstance(eos_token_id, int):\n",
    "        eos_token_id = [eos_token_id]\n",
    "    eos_token_id_tensor = torch.tensor(eos_token_id).to(input_ids.device) if eos_token_id is not None else None\n",
    "    output_scores = output_scores if output_scores is not None else self.generation_config.output_scores\n",
    "    output_attentions = (\n",
    "        output_attentions if output_attentions is not None else self.generation_config.output_attentions\n",
    "    )\n",
    "    output_hidden_states = (\n",
    "        output_hidden_states if output_hidden_states is not None else self.generation_config.output_hidden_states\n",
    "    )\n",
    "    return_dict_in_generate = (\n",
    "        return_dict_in_generate\n",
    "        if return_dict_in_generate is not None\n",
    "        else self.generation_config.return_dict_in_generate\n",
    "    )\n",
    "\n",
    "    # init attention / hidden states / scores tuples\n",
    "    raw_logits = () if (return_dict_in_generate and output_logits) else None\n",
    "    scores = () if (return_dict_in_generate and output_scores) else None\n",
    "    before = () if (return_dict_in_generate) else None\n",
    "    decoder_attentions = () if (return_dict_in_generate and output_attentions) else None\n",
    "    cross_attentions = () if (return_dict_in_generate and output_attentions) else None\n",
    "    decoder_hidden_states = () if (return_dict_in_generate and output_hidden_states) else None\n",
    "\n",
    "    # if model is an encoder-decoder, retrieve encoder attention weights and hidden states\n",
    "    if return_dict_in_generate and self.config.is_encoder_decoder:\n",
    "        encoder_attentions = model_kwargs[\"encoder_outputs\"].get(\"attentions\") if output_attentions else None\n",
    "        encoder_hidden_states = (\n",
    "            model_kwargs[\"encoder_outputs\"].get(\"hidden_states\") if output_hidden_states else None\n",
    "        )\n",
    "\n",
    "    # keep track of which sequences are already finished\n",
    "    batch_size, cur_len = input_ids.shape\n",
    "    if \"inputs_embeds\" in model_kwargs:\n",
    "        cur_len = model_kwargs[\"inputs_embeds\"].shape[1]\n",
    "    this_peer_finished = False\n",
    "    unfinished_sequences = torch.ones(batch_size, dtype=torch.long, device=input_ids.device)\n",
    "    model_kwargs[\"cache_position\"] = torch.arange(cur_len, device=input_ids.device)\n",
    "    \n",
    "    while self._has_unfinished_sequences(this_peer_finished, synced_gpus, device=input_ids.device):\n",
    "        # prepare model inputs\n",
    "        model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)\n",
    "        import pdb\n",
    "        # \n",
    "        if 'Scal' not in str(type(self)):\n",
    "            outputs = self(\n",
    "                **model_inputs,\n",
    "               \n",
    "                return_dict=True,\n",
    "                output_attentions=output_attentions,\n",
    "                output_hidden_states=output_hidden_states,\n",
    "            )\n",
    "        else:\n",
    "            \n",
    "            outputs = self(\n",
    "                **model_inputs,\n",
    "                weight=weight,\n",
    "                adjust_method=adjust_method,\n",
    "                pos=pos,\n",
    "                return_dict=True,\n",
    "                output_attentions=output_attentions,\n",
    "                output_hidden_states=output_hidden_states,\n",
    "            )\n",
    "\n",
    "        if synced_gpus and this_peer_finished:\n",
    "            continue  # don't waste resources running the code we don't need\n",
    "\n",
    "        next_token_logits = outputs.logits[:, -1, :]\n",
    "\n",
    "        # pre-process distribution\n",
    "        next_tokens_scores = logits_processor(input_ids, next_token_logits)\n",
    "\n",
    "        # Store scores, attentions and hidden_states when required\n",
    "        if return_dict_in_generate:\n",
    "            if output_scores:\n",
    "                scores += (next_tokens_scores,)\n",
    "            if output_logits:\n",
    "                raw_logits += (next_token_logits,)\n",
    "            if output_attentions:\n",
    "                decoder_attentions += (\n",
    "                    (outputs.decoder_attentions,) if self.config.is_encoder_decoder else (outputs.attentions,)\n",
    "                )\n",
    "                if self.config.is_encoder_decoder:\n",
    "                    cross_attentions += (outputs.cross_attentions,)\n",
    "\n",
    "            if output_hidden_states:\n",
    "                decoder_hidden_states += (\n",
    "                    (outputs.decoder_hidden_states,)\n",
    "                    if self.config.is_encoder_decoder\n",
    "                    else (outputs.hidden_states,)\n",
    "                )\n",
    "\n",
    "        # argmax\n",
    "        next_tokens = torch.argmax(next_tokens_scores, dim=-1)\n",
    "\n",
    "        # finished sentences should have their next token be a padding token\n",
    "        if eos_token_id is not None:\n",
    "            if pad_token_id is None:\n",
    "                raise ValueError(\"If `eos_token_id` is defined, make sure that `pad_token_id` is defined.\")\n",
    "            next_tokens = next_tokens * unfinished_sequences + pad_token_id * (1 - unfinished_sequences)\n",
    "\n",
    "        # update generated ids, model inputs, and length for next step\n",
    "        input_ids = torch.cat([input_ids, next_tokens[:, None]], dim=-1)\n",
    "        if streamer is not None:\n",
    "            streamer.put(next_tokens.cpu())\n",
    "        model_kwargs = self._update_model_kwargs_for_generation(\n",
    "            outputs,\n",
    "            model_kwargs,\n",
    "            is_encoder_decoder=self.config.is_encoder_decoder,\n",
    "        )\n",
    "\n",
    "        # if eos_token was found in one sentence, set sentence to finished\n",
    "        if eos_token_id_tensor is not None:\n",
    "            unfinished_sequences = unfinished_sequences.mul(\n",
    "                next_tokens.tile(eos_token_id_tensor.shape[0], 1).ne(eos_token_id_tensor.unsqueeze(1)).prod(dim=0)\n",
    "            )\n",
    "\n",
    "        unfinished_sequences = unfinished_sequences & ~stopping_criteria(input_ids, scores)\n",
    "        this_peer_finished = unfinished_sequences.max() == 0\n",
    "\n",
    "    if streamer is not None:\n",
    "        streamer.end()\n",
    "\n",
    "    if return_dict_in_generate:\n",
    "        if self.config.is_encoder_decoder:\n",
    "            return GenerateEncoderDecoderOutput(\n",
    "                sequences=input_ids,\n",
    "                scores=scores,\n",
    "                logits=raw_logits,\n",
    "                encoder_attentions=encoder_attentions,\n",
    "                encoder_hidden_states=encoder_hidden_states,\n",
    "                decoder_attentions=decoder_attentions,\n",
    "                cross_attentions=cross_attentions,\n",
    "                decoder_hidden_states=decoder_hidden_states,\n",
    "                past_key_values=model_kwargs.get(\"past_key_values\"),\n",
    "            )\n",
    "        else:\n",
    "            return GenerateDecoderOnlyOutput(\n",
    "                sequences=input_ids,\n",
    "                scores=scores,\n",
    "                logits=raw_logits,\n",
    "                attentions=decoder_attentions,\n",
    "                hidden_states=decoder_hidden_states,\n",
    "                past_key_values=model_kwargs.get(\"past_key_values\"),\n",
    "            )\n",
    "    else:\n",
    "        return input_ids\n",
    "    \n",
    "def change_greedy_to_add_weight():\n",
    "    transformers.generation.utils.GenerationMixin._greedy_search = _add_weight_greedy_search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98d0dfb4",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "where(): argument 'condition' (position 1) must be Tensor, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m keys = [torch.where(input_id == \u001b[32m32001\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m input_id \u001b[38;5;129;01min\u001b[39;00m single_input[\u001b[33m'\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m'\u001b[39m]]\n\u001b[32m     11\u001b[39m change_greedy_to_add_weight()\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m output = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msingle_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     20\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m real_output = output[\u001b[33m'\u001b[39m\u001b[33msequences\u001b[39m\u001b[33m'\u001b[39m][\u001b[32m0\u001b[39m][\u001b[38;5;28mlen\u001b[39m(single_input[\u001b[33m'\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m'\u001b[39m][-\u001b[32m1\u001b[39m]):]\n\u001b[32m     22\u001b[39m gens = [processor.decode(token, skip_special_tokens=\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m real_output]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/env1/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/env1/lib/python3.11/site-packages/transformers/generation/utils.py:1527\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[39m\n\u001b[32m   1509\u001b[39m     result = \u001b[38;5;28mself\u001b[39m.assisted_decoding(\n\u001b[32m   1510\u001b[39m         input_ids,\n\u001b[32m   1511\u001b[39m         candidate_generator=candidate_generator,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1523\u001b[39m         **model_kwargs,\n\u001b[32m   1524\u001b[39m     )\n\u001b[32m   1525\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m generation_mode == GenerationMode.GREEDY_SEARCH:\n\u001b[32m   1526\u001b[39m     \u001b[38;5;66;03m# 11. run greedy search\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1527\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_greedy_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1528\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1529\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1530\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1531\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1532\u001b[39m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1533\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1534\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_logits\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43moutput_logits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1535\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1536\u001b[39m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1537\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1538\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1539\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1541\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m generation_mode == GenerationMode.CONTRASTIVE_SEARCH:\n\u001b[32m   1542\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model_kwargs[\u001b[33m\"\u001b[39m\u001b[33muse_cache\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 137\u001b[39m, in \u001b[36m_add_weight_greedy_search\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, output_logits, return_dict_in_generate, synced_gpus, weight, adjust_method, pos, streamer, **model_kwargs)\u001b[39m\n\u001b[32m    128\u001b[39m     outputs = \u001b[38;5;28mself\u001b[39m(\n\u001b[32m    129\u001b[39m         **model_inputs,\n\u001b[32m    130\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    133\u001b[39m         output_hidden_states=output_hidden_states,\n\u001b[32m    134\u001b[39m     )\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[43m        \u001b[49m\u001b[43madjust_method\u001b[49m\u001b[43m=\u001b[49m\u001b[43madjust_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpos\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    144\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    145\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    147\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[32m    148\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/env1/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1551\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1552\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1553\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/env1/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1557\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1558\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1559\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1560\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1561\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1562\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1564\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1565\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/AdaptVis/model_zoo/llava/modeling_llava_scal.py:458\u001b[39m, in \u001b[36mLlavaForConditionalGenerationScal.forward\u001b[39m\u001b[34m(self, input_ids, pixel_values, attention_mask, position_ids, past_key_values, inputs_embeds, vision_feature_layer, vision_feature_select_strategy, labels, use_cache, output_attentions, output_hidden_states, return_dict, keys, weight, pos, adjust_method, caption_length)\u001b[39m\n\u001b[32m    455\u001b[39m             position_ids = torch.sum(attention_mask, dim=\u001b[32m1\u001b[39m).unsqueeze(-\u001b[32m1\u001b[39m) - \u001b[32m1\u001b[39m\n\u001b[32m    456\u001b[39m             image_id=\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m458\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlanguage_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    459\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    460\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# position_ids=position_ids,\u001b[39;49;00m\n\u001b[32m    461\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    462\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    463\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    464\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    465\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    466\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    467\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimage_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    468\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    469\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpos\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    470\u001b[39m \u001b[43m    \u001b[49m\u001b[43madjust_method\u001b[49m\u001b[43m=\u001b[49m\u001b[43madjust_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    471\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcaption_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcaption_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    472\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    475\u001b[39m logits = outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    477\u001b[39m loss = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/env1/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1551\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1552\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1553\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/env1/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1557\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1558\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1559\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1560\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1561\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1562\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1564\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1565\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/AdaptVis/model_zoo/llama/modeling_llama_add_attn.py:872\u001b[39m, in \u001b[36mLLaMAForCausalLMScal.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, keys, pos, weight, caption_length, adjust_method)\u001b[39m\n\u001b[32m    868\u001b[39m return_dict = return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.use_return_dict\n\u001b[32m    871\u001b[39m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m872\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    873\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpos\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcaption_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcaption_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    885\u001b[39m \u001b[43m    \u001b[49m\u001b[43madjust_method\u001b[49m\u001b[43m=\u001b[49m\u001b[43madjust_method\u001b[49m\n\u001b[32m    886\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    888\u001b[39m hidden_states = outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    889\u001b[39m logits = \u001b[38;5;28mself\u001b[39m.lm_head(hidden_states)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/env1/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1551\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1552\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1553\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/env1/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1557\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1558\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1559\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1560\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1561\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1562\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1564\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1565\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/AdaptVis/model_zoo/llama/modeling_llama_add_attn.py:708\u001b[39m, in \u001b[36mLLaMAModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, keys, pos, weight, caption_length, adjust_method)\u001b[39m\n\u001b[32m    701\u001b[39m     layer_outputs = torch.utils.checkpoint.checkpoint(\n\u001b[32m    702\u001b[39m         create_custom_forward(decoder_layer),\n\u001b[32m    703\u001b[39m         hidden_states,\n\u001b[32m    704\u001b[39m         attention_mask,\n\u001b[32m    705\u001b[39m         \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    706\u001b[39m     )\n\u001b[32m    707\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m708\u001b[39m     layer_outputs = \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    710\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    711\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    712\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    713\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    714\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    715\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    716\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpos\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    717\u001b[39m \u001b[43m        \u001b[49m\u001b[43midx\u001b[49m\u001b[43m=\u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    718\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcaption_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcaption_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    719\u001b[39m \u001b[43m        \u001b[49m\u001b[43madjust_method\u001b[49m\u001b[43m=\u001b[49m\u001b[43madjust_method\u001b[49m\n\u001b[32m    720\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    722\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    724\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/env1/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1551\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1552\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1553\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/env1/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1557\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1558\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1559\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1560\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1561\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1562\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1564\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1565\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/AdaptVis/model_zoo/llama/modeling_llama_add_attn.py:389\u001b[39m, in \u001b[36mLLaMADecoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, output_attentions, use_cache, past_key_value, keys, pos, weight, idx, caption_length, adjust_method)\u001b[39m\n\u001b[32m    386\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.input_layernorm(hidden_states)\n\u001b[32m    388\u001b[39m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m389\u001b[39m hidden_states, self_attn_weights, present_key_value = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    390\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    391\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    392\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    393\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    394\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    395\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpos\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    396\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    397\u001b[39m \u001b[43m    \u001b[49m\u001b[43midx\u001b[49m\u001b[43m=\u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    398\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcaption_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcaption_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[43m    \u001b[49m\u001b[43madjust_method\u001b[49m\u001b[43m=\u001b[49m\u001b[43madjust_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    402\u001b[39m hidden_states = residual + hidden_states\n\u001b[32m    404\u001b[39m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/env1/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1551\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1552\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1553\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/env1/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1557\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1558\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1559\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1560\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1561\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1562\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1564\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1565\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/AdaptVis/model_zoo/llama/modeling_llama_add_attn.py:271\u001b[39m, in \u001b[36mLLaMAAttention.forward\u001b[39m\u001b[34m(self, hidden_states, past_key_value, attention_mask, position_ids, use_cache, output_attentions, output_head_hidden_states, keys, weight, pos, idx, caption_length, adjust_method)\u001b[39m\n\u001b[32m    269\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m idx<\u001b[32m32\u001b[39m:\n\u001b[32m    270\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attn_weights.size()[\u001b[32m2\u001b[39m]==attn_weights.size()[\u001b[32m3\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m271\u001b[39m         true_indices = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;28;01mTrue\u001b[39;00m]\n\u001b[32m    272\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(true_indices) == \u001b[32m0\u001b[39m:\n\u001b[32m    273\u001b[39m             \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mNo True values found in index.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: where(): argument 'condition' (position 1) must be Tensor, not NoneType"
     ]
    }
   ],
   "source": [
    "query = \"Hello, my name is \"\n",
    "prompt = f\"USER: {query}\\nASSISTANT:\"\n",
    "single_input = processor.tokenizer(\n",
    "    text=prompt,\n",
    "    padding=\"max_length\", \n",
    "    return_tensors=\"pt\", \n",
    "    max_length=77\n",
    ").to(device)\n",
    "\n",
    "keys = [torch.where(input_id == 32001, 1, 0) for input_id in single_input['input_ids']]\n",
    "change_greedy_to_add_weight()\n",
    "\n",
    "output = model.generate(\n",
    "    **single_input,\n",
    "    keys=keys,\n",
    "    weight=1.0,\n",
    "    max_new_tokens=100, \n",
    "    output_scores=True, \n",
    "    return_dict_in_generate=True\n",
    ")\n",
    "real_output = output['sequences'][0][len(single_input['input_ids'][-1]):]\n",
    "gens = [processor.decode(token, skip_special_tokens=True) for token in real_output]\n",
    "gen = processor.decode(real_output, skip_special_tokens=True).strip()\n",
    "probs = [torch.log_softmax(s[0], dim=-1).max().item() for s in output['scores']]\n",
    "confidence = float(np.mean(probs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75f56244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/venv/env1/lib/python3.11/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████████████| 3/3 [00:04<00:00,  1.38s/it]\n",
      "Some weights of LlavaForConditionalGenerationScal were not initialized from the model checkpoint at llava-hf/llava-1.5-7b-hf and are newly initialized: ['language_model.model.layers.0.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.1.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.10.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.11.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.12.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.13.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.14.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.15.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.16.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.17.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.18.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.19.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.2.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.20.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.21.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.22.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.23.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.24.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.25.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.26.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.27.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.28.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.29.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.3.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.30.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.31.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.4.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.5.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.6.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.7.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.8.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.9.self_attn.rotary_emb.inv_freq']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Controlled_Images_A llava1.5\n",
      "  0%|                                                   | 0/330 [00:00<?, ?it/s]Prompt: <image>\n",
      "USER: Where is the beer bottle in relation to the armchair? Answer with left, right, on or under.\n",
      "ASSISTANT:\n",
      "Generation: Left\n",
      "Golden: On\n",
      "0 1 0.0\n",
      "  0%|▏                                          | 1/330 [00:06<36:45,  6.70s/it]Prompt: <image>\n",
      "USER: Where is the scarf in relation to the armchair? Answer with left, right, on or under.\n",
      "ASSISTANT:\n",
      "Generation: Right\n",
      "Golden: Right\n",
      "1 2 0.5\n",
      "  1%|▎                                          | 2/330 [00:09<22:29,  4.11s/it]Prompt: <image>\n",
      "USER: Where is the orange in relation to the armchair? Answer with left, right, on or under.\n",
      "ASSISTANT:\n",
      "Generation: Right\n",
      "Golden: Right\n",
      "2 3 0.6666666666666666\n",
      "  1%|▍                                          | 3/330 [00:11<18:42,  3.43s/it]Prompt: <image>\n",
      "USER: Where is the scarf in relation to the table? Answer with left, right, on or under.\n",
      "ASSISTANT:\n",
      "Generation: Under\n",
      "Golden: Under\n",
      "3 4 0.75\n",
      "  1%|▌                                          | 4/330 [00:13<15:57,  2.94s/it]Prompt: <image>\n",
      "USER: Where are the sunglasses in relation to the chair? Answer with left, right, on or under.\n",
      "ASSISTANT:\n",
      "Generation: Left\n",
      "Golden: Left\n",
      "4 5 0.8\n",
      "  2%|▋                                          | 5/330 [00:16<14:43,  2.72s/it]Prompt: <image>\n",
      "USER: Where is the toilet roll in relation to the table? Answer with left, right, on or under.\n",
      "ASSISTANT:\n",
      "Generation: Right\n",
      "Golden: Left\n",
      "4 6 0.6666666666666666\n",
      "  2%|▊                                          | 6/330 [00:18<13:52,  2.57s/it]Prompt: <image>\n",
      "USER: Where is the bottle in relation to the chair? Answer with left, right, on or under.\n",
      "ASSISTANT:\n",
      "Generation: Right\n",
      "Golden: Right\n",
      "5 7 0.7142857142857143\n",
      "  2%|▉                                          | 7/330 [00:20<13:30,  2.51s/it]Prompt: <image>\n",
      "USER: Where is the cup in relation to the chair? Answer with left, right, on or under.\n",
      "ASSISTANT:\n",
      "Generation: Right\n",
      "Golden: Right\n",
      "6 8 0.75\n",
      "  2%|█                                          | 8/330 [00:23<12:58,  2.42s/it]Prompt: <image>\n",
      "USER: Where is the bowl in relation to the armchair? Answer with left, right, on or under.\n",
      "ASSISTANT:\n",
      "Generation: Left\n",
      "Golden: Left\n",
      "7 9 0.7777777777777778\n",
      "  3%|█▏                                         | 9/330 [00:25<12:53,  2.41s/it]Prompt: <image>\n",
      "USER: Where is the mug in relation to the table? Answer with left, right, on or under.\n",
      "ASSISTANT:\n",
      "Generation: Right\n",
      "Golden: Right\n",
      "8 10 0.8\n",
      "  3%|█▎                                        | 10/330 [00:27<12:51,  2.41s/it]Prompt: <image>\n",
      "USER: Where is the mug in relation to the table? Answer with left, right, on or under.\n",
      "ASSISTANT:\n",
      "Generation: Right\n",
      "Golden: On\n",
      "8 11 0.7272727272727273\n",
      "  3%|█▍                                        | 11/330 [00:30<12:37,  2.37s/it]Prompt: <image>\n",
      "USER: Where is the cup in relation to the armchair? Answer with left, right, on or under.\n",
      "ASSISTANT:\n",
      "Generation: Right\n",
      "Golden: On\n",
      "8 12 0.6666666666666666\n",
      "  4%|█▌                                        | 12/330 [00:32<12:28,  2.36s/it]Prompt: <image>\n",
      "USER: Where are the shoes in relation to the chair? Answer with left, right, on or under.\n",
      "ASSISTANT:\n",
      "Generation: Left\n",
      "Golden: Left\n",
      "9 13 0.6923076923076923\n",
      "  4%|█▋                                        | 13/330 [00:34<12:22,  2.34s/it]Prompt: <image>\n",
      "USER: Where is the orange in relation to the chair? Answer with left, right, on or under.\n",
      "ASSISTANT:\n",
      "Generation: Right\n",
      "Golden: Right\n",
      "10 14 0.7142857142857143\n",
      "  4%|█▊                                        | 14/330 [00:37<12:13,  2.32s/it]Prompt: <image>\n",
      "USER: Where is the helmet in relation to the table? Answer with left, right, on or under.\n",
      "ASSISTANT:\n",
      "Generation: Right\n",
      "Golden: Right\n",
      "11 15 0.7333333333333333\n",
      "  5%|█▉                                        | 15/330 [00:39<12:08,  2.31s/it]Prompt: <image>\n",
      "USER: Where is the water filter in relation to the chair? Answer with left, right, on or under.\n",
      "ASSISTANT:\n",
      "Generation: Right\n",
      "Golden: Right\n",
      "12 16 0.75\n",
      "  5%|██                                        | 16/330 [00:41<12:07,  2.32s/it]Prompt: <image>\n",
      "USER: Where is the glove in relation to the armchair? Answer with left, right, on or under.\n",
      "ASSISTANT:\n",
      "Generation: Left\n",
      "Golden: Left\n",
      "13 17 0.7647058823529411\n",
      "  5%|██▏                                       | 17/330 [00:44<12:10,  2.34s/it]Prompt: <image>\n",
      "USER: Where is the dragonfruit in relation to the table? Answer with left, right, on or under.\n",
      "ASSISTANT:\n",
      "Generation: Right\n",
      "Golden: Left\n",
      "13 18 0.7222222222222222\n",
      "  5%|██▎                                       | 18/330 [00:46<12:06,  2.33s/it]Prompt: <image>\n",
      "USER: Where is the scarf in relation to the table? Answer with left, right, on or under.\n",
      "ASSISTANT:\n",
      "Generation: Right\n",
      "Golden: Right\n",
      "14 19 0.7368421052631579\n",
      "  6%|██▍                                       | 19/330 [00:48<12:01,  2.32s/it]Prompt: <image>\n",
      "USER: Where is the water filter in relation to the chair? Answer with left, right, on or under.\n",
      "ASSISTANT:\n",
      "Generation: Left\n",
      "Golden: On\n",
      "14 20 0.7\n",
      "  6%|██▌                                       | 20/330 [00:53<15:23,  2.98s/it]Prompt: <image>\n",
      "USER: Where is the dragonfruit in relation to the chair? Answer with left, right, on or under.\n",
      "ASSISTANT:\n",
      "Generation: Left\n",
      "Golden: Left\n",
      "15 21 0.7142857142857143\n",
      "  6%|██▋                                       | 21/330 [00:55<14:13,  2.76s/it]Prompt: <image>\n",
      "USER: Where is the phone in relation to the table? Answer with left, right, on or under.\n",
      "ASSISTANT:\n",
      "Generation: Left\n",
      "Golden: Left\n",
      "16 22 0.7272727272727273\n",
      "  7%|██▊                                       | 22/330 [00:57<13:38,  2.66s/it]Prompt: <image>\n",
      "USER: Where is the pan in relation to the chair? Answer with left, right, on or under.\n",
      "ASSISTANT:\n",
      "Generation: Right\n",
      "Golden: On\n",
      "16 23 0.6956521739130435\n",
      "  7%|██▉                                       | 23/330 [01:00<13:05,  2.56s/it]Prompt: <image>\n",
      "USER: Where is the cap in relation to the table? Answer with left, right, on or under.\n",
      "ASSISTANT:\n",
      "Generation: Right\n",
      "Golden: Right\n",
      "17 24 0.7083333333333334\n",
      "  7%|███                                       | 24/330 [01:02<12:43,  2.49s/it]Prompt: <image>\n",
      "USER: Where is the yarn in relation to the armchair? Answer with left, right, on or under.\n",
      "ASSISTANT:\n",
      "Generation: Left\n",
      "Golden: Under\n",
      "17 25 0.68\n",
      "  8%|███▏                                      | 25/330 [01:07<17:12,  3.38s/it]Prompt: <image>\n",
      "USER: Where is the banana in relation to the table? Answer with left, right, on or under.\n",
      "ASSISTANT:\n",
      "Generation: Left\n",
      "Golden: On\n",
      "17 26 0.6538461538461539\n",
      "  8%|███▎                                      | 26/330 [01:12<18:57,  3.74s/it]Prompt: <image>\n",
      "USER: Where is the orange in relation to the armchair? Answer with left, right, on or under.\n",
      "ASSISTANT:\n",
      "Generation: Left\n",
      "Golden: On\n",
      "17 27 0.6296296296296297\n",
      "  8%|███▍                                      | 27/330 [01:17<20:56,  4.15s/it]Prompt: <image>\n",
      "USER: Where is the rose in relation to the armchair? Answer with left, right, on or under.\n",
      "ASSISTANT:\n",
      "Generation: Left\n",
      "Golden: Left\n",
      "18 28 0.6428571428571429\n",
      "  8%|███▌                                      | 28/330 [01:19<18:11,  3.61s/it]Prompt: <image>\n",
      "USER: Where is the water bottle in relation to the armchair? Answer with left, right, on or under.\n",
      "ASSISTANT:\n",
      "Generation: Left\n",
      "Golden: Left\n",
      "19 29 0.6551724137931034\n",
      "  9%|███▋                                      | 29/330 [01:22<16:10,  3.23s/it]Prompt: <image>\n",
      "USER: Where is the lemon in relation to the table? Answer with left, right, on or under.\n",
      "ASSISTANT:\n",
      "Generation: Left\n",
      "Golden: Left\n",
      "20 30 0.6666666666666666\n",
      "  9%|███▊                                      | 30/330 [01:26<17:36,  3.52s/it]Prompt: <image>\n",
      "USER: Where is the bottle in relation to the armchair? Answer with left, right, on or under.\n",
      "ASSISTANT:\n",
      "Generation: Left\n",
      "Golden: Left\n",
      "21 31 0.6774193548387096\n",
      "  9%|███▉                                      | 31/330 [01:28<15:53,  3.19s/it]Prompt: <image>\n",
      "USER: Where is the plate in relation to the armchair? Answer with left, right, on or under.\n",
      "ASSISTANT:\n",
      "Generation: Left\n",
      "Golden: On\n",
      "21 32 0.65625\n",
      " 10%|████                                      | 32/330 [01:34<18:40,  3.76s/it]^C\n",
      "Process Process-15:\n",
      "Process Process-13:\n",
      "Process Process-12:\n",
      "Process Process-16:\n",
      "Process Process-10:\n",
      "Process Process-11:\n",
      "Process Process-7:\n",
      "Process Process-8:\n",
      "Process Process-6:\n",
      "Process Process-2:\n",
      "Process Process-9:\n",
      "Process Process-5:\n",
      "Process Process-14:\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7c4170592840>\n",
      "Traceback (most recent call last):\n",
      "  File \"/venv/env1/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 1477, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/venv/env1/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 1441, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/venv/env1/lib/python3.11/multiprocessing/process.py\", line 149, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/venv/env1/lib/python3.11/multiprocessing/popen_fork.py\", line 40, in wait\n",
      "    if not wait([self.sentinel], timeout):\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/venv/env1/lib/python3.11/multiprocessing/connection.py\", line 948, in wait\n",
      "    ready = selector.select(timeout)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/venv/env1/lib/python3.11/selectors.py\", line 415, in select\n",
      "    fd_event_list = self._selector.poll(timeout)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt: \n",
      " 10%|████                                      | 32/330 [01:36<14:55,  3.00s/it]\n",
      "Traceback (most recent call last):\n",
      "  File \"/workspace/AdaptVis/main_aro.py\", line 93, in <module>\n",
      "    main(args)\n",
      "  File \"/workspace/AdaptVis/main_aro.py\", line 78, in main\n",
      "    scores, correct_id = model.get_out_scores_wh_batched(args.dataset,joint_loader,args.method,args.weight,args.option,args.threshold,args.weight1,args.weight2)\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/venv/env1/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspace/AdaptVis/model_zoo/llava15.py\", line 454, in get_out_scores_wh_batched\n",
      "    result = self.adaptvis_bidirectional_reasoning(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/venv/env1/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspace/AdaptVis/model_zoo/llava15.py\", line 872, in adaptvis_bidirectional_reasoning\n",
      "    output_reverse, results_reverse = self._generate(image, query_reverse, weight=selected_weight)\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspace/AdaptVis/model_zoo/llava15.py\", line 784, in _generate\n",
      "    output = self.model.generate(\n",
      "             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/venv/env1/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/venv/env1/lib/python3.11/site-packages/transformers/generation/utils.py\", line 1527, in generate\n",
      "    result = self._greedy_search(\n",
      "             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspace/AdaptVis/model_zoo/llava15.py\", line 139, in _add_weight_greedy_search\n",
      "    outputs = self(\n",
      "              ^^^^^\n",
      "  File \"/venv/env1/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/venv/env1/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspace/AdaptVis/model_zoo/llava/modeling_llava_scal.py\", line 452, in forward\n",
      "    outputs = self.language_model(\n",
      "              ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/venv/env1/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/venv/env1/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspace/AdaptVis/model_zoo/llama/modeling_llama_add_attn.py\", line 872, in forward\n",
      "    outputs = self.model(\n",
      "              ^^^^^^^^^^^\n",
      "  File \"/venv/env1/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/venv/env1/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspace/AdaptVis/model_zoo/llama/modeling_llama_add_attn.py\", line 708, in forward\n",
      "    layer_outputs = decoder_layer(\n",
      "                    ^^^^^^^^^^^^^^\n",
      "  File \"/venv/env1/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/venv/env1/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspace/AdaptVis/model_zoo/llama/modeling_llama_add_attn.py\", line 389, in forward\n",
      "    hidden_states, self_attn_weights, present_key_value = self.self_attn(\n",
      "                                                          ^^^^^^^^^^^^^^^\n",
      "  File \"/venv/env1/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/venv/env1/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspace/AdaptVis/model_zoo/llama/modeling_llama_add_attn.py\", line 246, in forward\n",
      "    cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/venv/env1/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/venv/env1/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspace/AdaptVis/model_zoo/llama/modeling_llama_add_attn.py\", line 131, in forward\n",
      "    self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype, device=x.device),\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!bash my_run.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6426621",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "prompts_path = 'prompts/Controlled_Images_A_with_answer_four_options.jsonl'\n",
    "image_paths_path = 'data/controlled_images_dataset.json'\n",
    "\n",
    "prompts = []\n",
    "image_paths = None\n",
    "\n",
    "with open(prompts_path) as f:\n",
    "    for line in f:\n",
    "        prompts.append(json.loads(line))\n",
    "with open(image_paths_path) as f:\n",
    "    image_paths = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d68610e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def merge_lists(list1, list2):\n",
    "    merged = []\n",
    "    pattern = re.compile(r\"<image>\\nUSER:(.*)\\nASSISTANT:\", re.DOTALL)\n",
    "\n",
    "    for d1, d2 in zip(list1, list2):\n",
    "        q = d1[\"question\"]\n",
    "        m = pattern.search(q)\n",
    "        if not m:\n",
    "            raise ValueError(f\"Question format unexpected: {q!r}\")\n",
    "        question_text = m.group(1).strip()\n",
    "\n",
    "        merged.append(\n",
    "            {\n",
    "                \"id\": d1[\"id\"],\n",
    "                \"image_path\": d2[\"image_path\"],\n",
    "                \"question\": question_text,\n",
    "                \"answer_label\": d1[\"answer\"][0],\n",
    "            }\n",
    "        )\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1adbbe61",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = merge_lists(prompts, image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18353c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"merged_cont_A.jsonl\", 'w') as f:\n",
    "    for data in merged:\n",
    "        f.write(json.dumps(data) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ef89088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ 'mode' argument is missing. Valid modes are ['val', 'export', 'train', 'predict', 'track', 'benchmark']. Using default 'mode=train'.\n",
      "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8x.pt to 'yolov8x.pt': 100% ━━━━━━━━━━━━ 130.5MB 139.3MB/s 0.9ss 0.9s<0.0s\n",
      "WARNING ⚠️ 'data' argument is missing. Using default 'data=coco8.yaml'.\n",
      "Ultralytics 8.3.230 🚀 Python-3.11.14 torch-2.4.1+cu121 CUDA:0 (NVIDIA GeForce RTX 4090, 48519MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=coco8.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=100, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8x.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=train, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/workspace/AdaptVis/runs/detect/train, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=https://ultralytics.com/images/zidane.jpg, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "\n",
      "WARNING ⚠️ Dataset 'coco8.yaml' images not found, missing path '/workspace/AdaptVis/datasets/coco8/images/val'\n",
      "\u001b[KDownloading https://ultralytics.com/assets/coco8.zip to '/workspace/AdaptVis/datasets/coco8.zip': 100% ━━━━━━━━━━━━ 432.8KB 64.6MB/s 0.0s\n",
      "\u001b[KUnzipping /workspace/AdaptVis/datasets/coco8.zip to /workspace/AdaptVis/datasets/coco8...: 100% ━━━━━━━━━━━━ 25/25 2.4Kfiles/s 0.0s\n",
      "Dataset download success ✅ (0.2s), saved to \u001b[1m/workspace/AdaptVis/datasets\u001b[0m\n",
      "\n",
      "\u001b[KDownloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf': 100% ━━━━━━━━━━━━ 755.1KB 93.1MB/s 0.0s\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      2320  ultralytics.nn.modules.conv.Conv             [3, 80, 3, 2]                 \n",
      "  1                  -1  1    115520  ultralytics.nn.modules.conv.Conv             [80, 160, 3, 2]               \n",
      "  2                  -1  3    436800  ultralytics.nn.modules.block.C2f             [160, 160, 3, True]           \n",
      "  3                  -1  1    461440  ultralytics.nn.modules.conv.Conv             [160, 320, 3, 2]              \n",
      "  4                  -1  6   3281920  ultralytics.nn.modules.block.C2f             [320, 320, 6, True]           \n",
      "  5                  -1  1   1844480  ultralytics.nn.modules.conv.Conv             [320, 640, 3, 2]              \n",
      "  6                  -1  6  13117440  ultralytics.nn.modules.block.C2f             [640, 640, 6, True]           \n",
      "  7                  -1  1   3687680  ultralytics.nn.modules.conv.Conv             [640, 640, 3, 2]              \n",
      "  8                  -1  3   6969600  ultralytics.nn.modules.block.C2f             [640, 640, 3, True]           \n",
      "  9                  -1  1   1025920  ultralytics.nn.modules.block.SPPF            [640, 640, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  3   7379200  ultralytics.nn.modules.block.C2f             [1280, 640, 3]                \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  3   1948800  ultralytics.nn.modules.block.C2f             [960, 320, 3]                 \n",
      " 16                  -1  1    922240  ultralytics.nn.modules.conv.Conv             [320, 320, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  3   7174400  ultralytics.nn.modules.block.C2f             [960, 640, 3]                 \n",
      " 19                  -1  1   3687680  ultralytics.nn.modules.conv.Conv             [640, 640, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  3   7379200  ultralytics.nn.modules.block.C2f             [1280, 640, 3]                \n",
      " 22        [15, 18, 21]  1   8795008  ultralytics.nn.modules.head.Detect           [80, [320, 640, 640]]         \n",
      "Model summary: 209 layers, 68,229,648 parameters, 68,229,632 gradients, 258.5 GFLOPs\n",
      "\n",
      "Transferred 595/595 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt to 'yolo11n.pt': 100% ━━━━━━━━━━━━ 5.4MB 169.0MB/s 0.0s\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 1186.7±302.2 MB/s, size: 50.0 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /workspace/AdaptVis/datasets/coco8/labels/train... 4 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 4/4 790.2it/s 0.0s\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /workspace/AdaptVis/datasets/coco8/labels/train.cache\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 851.0±412.5 MB/s, size: 54.0 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /workspace/AdaptVis/datasets/coco8/labels/val... 4 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 4/4 865.6it/s 0.0s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /workspace/AdaptVis/datasets/coco8/labels/val.cache\n",
      "Plotting labels to /workspace/AdaptVis/runs/detect/train/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000119, momentum=0.9) with parameter groups 97 weight(decay=0.0), 104 weight(decay=0.0005), 103 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1m/workspace/AdaptVis/runs/detect/train\u001b[0m\n",
      "Starting training for 100 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      1/100      3.54G     0.8528      1.193      1.366         18        640: 100% ━━━━━━━━━━━━ 1/1 1.7it/s 0.6s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 1.3it/s 0.8s\n",
      "                   all          4         17      0.838      0.949      0.977      0.773\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      2/100      3.64G     0.8904      2.074      1.373         22        640: 100% ━━━━━━━━━━━━ 1/1 10.1it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 14.4it/s 0.1s\n",
      "                   all          4         17      0.832       0.95      0.977      0.774\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      3/100      3.64G     0.8475      1.724      1.327         24        640: 100% ━━━━━━━━━━━━ 1/1 8.2it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 10.5it/s 0.1s\n",
      "                   all          4         17      0.848      0.921      0.976      0.773\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      4/100      3.65G     0.8177      1.521      1.252         34        640: 100% ━━━━━━━━━━━━ 1/1 6.8it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 27.9it/s 0.0s\n",
      "                   all          4         17       0.83      0.948      0.979      0.778\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      5/100       3.9G      1.113      1.599       1.61         24        640: 100% ━━━━━━━━━━━━ 1/1 7.0it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 26.9it/s 0.0s\n",
      "                   all          4         17      0.825      0.948      0.979      0.777\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      6/100       3.9G     0.7164     0.9586      1.225         25        640: 100% ━━━━━━━━━━━━ 1/1 7.3it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 28.4it/s 0.0s\n",
      "                   all          4         17      0.821       0.95       0.98      0.762\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      7/100       3.9G     0.9783      1.605      1.373         16        640: 100% ━━━━━━━━━━━━ 1/1 7.7it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 29.1it/s 0.0s\n",
      "                   all          4         17       0.82      0.947      0.979      0.769\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      8/100       3.9G      1.194      2.153      1.779         15        640: 100% ━━━━━━━━━━━━ 1/1 7.6it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 28.8it/s 0.0s\n",
      "                   all          4         17      0.817      0.946      0.979       0.77\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      9/100       3.9G     0.9977      1.154      1.322         27        640: 100% ━━━━━━━━━━━━ 1/1 6.9it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 26.6it/s 0.0s\n",
      "                   all          4         17      0.879      0.885      0.979      0.779\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     10/100       3.9G     0.7654      1.174      1.431         22        640: 100% ━━━━━━━━━━━━ 1/1 6.1it/s 0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 19.4it/s 0.1s\n",
      "                   all          4         17      0.878      0.883      0.979      0.768\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     11/100       3.9G     0.7917      0.464      1.218         17        640: 100% ━━━━━━━━━━━━ 1/1 7.7it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 29.1it/s 0.0s\n",
      "                   all          4         17      0.876      0.882      0.979      0.769\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     12/100       3.9G     0.7965     0.9355      1.242         44        640: 100% ━━━━━━━━━━━━ 1/1 7.2it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 26.8it/s 0.0s\n",
      "                   all          4         17      0.878       0.88      0.979      0.768\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     13/100       3.9G     0.6619     0.8926       1.14         33        640: 100% ━━━━━━━━━━━━ 1/1 7.6it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 27.4it/s 0.0s\n",
      "                   all          4         17      0.883      0.879      0.979      0.766\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     14/100       3.9G     0.7639     0.6455      1.258         29        640: 100% ━━━━━━━━━━━━ 1/1 6.3it/s 0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 17.4it/s 0.1s\n",
      "                   all          4         17      0.884      0.879      0.979      0.766\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     15/100       3.9G     0.7889     0.7545      1.224         28        640: 100% ━━━━━━━━━━━━ 1/1 7.5it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 28.4it/s 0.0s\n",
      "                   all          4         17      0.874      0.884      0.979      0.749\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     16/100       3.9G     0.7473     0.5777      1.232         36        640: 100% ━━━━━━━━━━━━ 1/1 7.7it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 30.8it/s 0.0s\n",
      "                   all          4         17      0.859      0.893      0.979      0.749\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     17/100       3.9G     0.5463       1.03      1.139         28        640: 100% ━━━━━━━━━━━━ 1/1 7.1it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 26.4it/s 0.0s\n",
      "                   all          4         17      0.781      0.859      0.951      0.756\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     18/100       3.9G     0.5856      0.658      1.069         18        640: 100% ━━━━━━━━━━━━ 1/1 8.1it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 19.8it/s 0.1s\n",
      "                   all          4         17      0.781      0.859      0.951      0.756\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     19/100      4.02G     0.6685      0.443      1.297         22        640: 100% ━━━━━━━━━━━━ 1/1 7.5it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 27.6it/s 0.0s\n",
      "                   all          4         17      0.757      0.871      0.951       0.74\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     20/100      4.02G     0.8306     0.7102      1.239         15        640: 100% ━━━━━━━━━━━━ 1/1 8.4it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 20.2it/s 0.0s\n",
      "                   all          4         17      0.757      0.871      0.951       0.74\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     21/100      4.02G     0.7479     0.6003      1.303         28        640: 100% ━━━━━━━━━━━━ 1/1 7.1it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 27.4it/s 0.0s\n",
      "                   all          4         17      0.723       0.88      0.951      0.757\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     22/100      4.02G     0.6208     0.5719      1.235         22        640: 100% ━━━━━━━━━━━━ 1/1 10.1it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 25.2it/s 0.0s\n",
      "                   all          4         17      0.723       0.88      0.951      0.757\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     23/100      4.02G      0.614     0.6669      1.207         19        640: 100% ━━━━━━━━━━━━ 1/1 6.8it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 25.7it/s 0.0s\n",
      "                   all          4         17      0.635        0.9      0.921      0.731\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     24/100      4.02G     0.4312       0.49      1.039         25        640: 100% ━━━━━━━━━━━━ 1/1 9.0it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 20.4it/s 0.0s\n",
      "                   all          4         17      0.635        0.9      0.921      0.731\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     25/100      4.02G     0.3775     0.4694     0.9625         20        640: 100% ━━━━━━━━━━━━ 1/1 7.2it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 28.1it/s 0.0s\n",
      "                   all          4         17      0.627        0.9      0.914      0.718\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     26/100      4.02G     0.6545     0.5263      1.087         47        640: 100% ━━━━━━━━━━━━ 1/1 8.8it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 17.7it/s 0.1s\n",
      "                   all          4         17      0.627        0.9      0.914      0.718\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     27/100      4.04G      0.477     0.5023      1.041         40        640: 100% ━━━━━━━━━━━━ 1/1 7.3it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 28.0it/s 0.0s\n",
      "                   all          4         17      0.902      0.641      0.914      0.719\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     28/100      4.04G     0.4742     0.3792      1.011         24        640: 100% ━━━━━━━━━━━━ 1/1 9.0it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 20.0it/s 0.0s\n",
      "                   all          4         17      0.902      0.641      0.914      0.719\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     29/100      4.04G     0.6789     0.4235      1.235         29        640: 100% ━━━━━━━━━━━━ 1/1 7.3it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 26.5it/s 0.0s\n",
      "                   all          4         17      0.902      0.639      0.911      0.716\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     30/100      4.04G     0.6009     0.3785      1.299         17        640: 100% ━━━━━━━━━━━━ 1/1 8.9it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 21.1it/s 0.0s\n",
      "                   all          4         17      0.902      0.639      0.911      0.716\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     31/100      4.04G     0.5951     0.4878      1.125         21        640: 100% ━━━━━━━━━━━━ 1/1 7.3it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 27.4it/s 0.0s\n",
      "                   all          4         17      0.896      0.634      0.911      0.732\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     32/100      4.04G     0.3764     0.3325      1.062         15        640: 100% ━━━━━━━━━━━━ 1/1 8.2it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 20.1it/s 0.0s\n",
      "                   all          4         17      0.896      0.634      0.911      0.732\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     33/100      4.04G     0.3619     0.3499     0.9962         23        640: 100% ━━━━━━━━━━━━ 1/1 7.4it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 28.3it/s 0.0s\n",
      "                   all          4         17      0.801      0.654      0.881      0.718\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     34/100      4.04G     0.5997     0.4938      1.065         26        640: 100% ━━━━━━━━━━━━ 1/1 8.1it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 19.4it/s 0.1s\n",
      "                   all          4         17      0.801      0.654      0.881      0.718\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     35/100      4.04G     0.3785     0.3013     0.9388         36        640: 100% ━━━━━━━━━━━━ 1/1 7.3it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 27.1it/s 0.0s\n",
      "                   all          4         17      0.797      0.655      0.879      0.734\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     36/100      4.04G     0.4664     0.3781      1.092         31        640: 100% ━━━━━━━━━━━━ 1/1 9.0it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 20.8it/s 0.0s\n",
      "                   all          4         17      0.797      0.655      0.879      0.734\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     37/100      4.04G     0.6355     0.6186      1.244         19        640: 100% ━━━━━━━━━━━━ 1/1 7.4it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 27.4it/s 0.0s\n",
      "                   all          4         17      0.899      0.633      0.906      0.747\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     38/100      4.04G     0.4837     0.3432     0.9547         49        640: 100% ━━━━━━━━━━━━ 1/1 8.9it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 21.0it/s 0.0s\n",
      "                   all          4         17      0.899      0.633      0.906      0.747\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     39/100      4.04G     0.3145     0.2789     0.8713         32        640: 100% ━━━━━━━━━━━━ 1/1 7.4it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 27.2it/s 0.0s\n",
      "                   all          4         17      0.893       0.63      0.907      0.747\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     40/100      4.04G     0.5178     0.5038      1.025         14        640: 100% ━━━━━━━━━━━━ 1/1 9.0it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 19.8it/s 0.1s\n",
      "                   all          4         17      0.893       0.63      0.907      0.747\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     41/100      4.04G     0.4494     0.2938      1.026         39        640: 100% ━━━━━━━━━━━━ 1/1 7.4it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 27.9it/s 0.0s\n",
      "                   all          4         17       0.89       0.63      0.904      0.759\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     42/100      4.04G     0.4578     0.3727       0.97         46        640: 100% ━━━━━━━━━━━━ 1/1 9.3it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 20.6it/s 0.0s\n",
      "                   all          4         17       0.89       0.63      0.904      0.759\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     43/100      4.04G     0.3764     0.3129     0.9927         17        640: 100% ━━━━━━━━━━━━ 1/1 7.5it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 28.2it/s 0.0s\n",
      "                   all          4         17      0.891      0.631      0.822      0.668\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     44/100      4.04G     0.5427     0.5532      1.034         47        640: 100% ━━━━━━━━━━━━ 1/1 9.1it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 19.9it/s 0.1s\n",
      "                   all          4         17      0.891      0.631      0.822      0.668\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     45/100      4.04G     0.3861     0.3049       1.02         32        640: 100% ━━━━━━━━━━━━ 1/1 7.4it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 25.7it/s 0.0s\n",
      "                   all          4         17      0.884      0.631      0.739      0.585\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     46/100      4.04G     0.3999     0.5493     0.9854         17        640: 100% ━━━━━━━━━━━━ 1/1 9.2it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 21.3it/s 0.0s\n",
      "                   all          4         17      0.884      0.631      0.739      0.585\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     47/100      4.04G      0.448     0.5817      1.064         18        640: 100% ━━━━━━━━━━━━ 1/1 7.4it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 27.6it/s 0.0s\n",
      "                   all          4         17      0.854       0.65      0.734      0.561\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     48/100      4.04G     0.4247     0.3992      1.026         20        640: 100% ━━━━━━━━━━━━ 1/1 8.8it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 20.2it/s 0.0s\n",
      "                   all          4         17      0.854       0.65      0.734      0.561\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     49/100      4.04G     0.4098     0.4789     0.9632         23        640: 100% ━━━━━━━━━━━━ 1/1 7.3it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 27.3it/s 0.0s\n",
      "                   all          4         17      0.849      0.651      0.734      0.559\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     50/100      4.04G     0.3892     0.3014      0.967         34        640: 100% ━━━━━━━━━━━━ 1/1 8.8it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 19.3it/s 0.1s\n",
      "                   all          4         17      0.849      0.651      0.734      0.559\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     51/100      4.04G     0.4874     0.3227     0.9943         34        640: 100% ━━━━━━━━━━━━ 1/1 8.2it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 28.6it/s 0.0s\n",
      "                   all          4         17       0.83      0.654      0.731      0.549\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     52/100      4.04G     0.4035     0.3753     0.9215         41        640: 100% ━━━━━━━━━━━━ 1/1 9.0it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 20.2it/s 0.0s\n",
      "                   all          4         17       0.83      0.654      0.731      0.549\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     53/100      4.04G     0.3511     0.3315     0.9706         22        640: 100% ━━━━━━━━━━━━ 1/1 8.9it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 20.3it/s 0.0s\n",
      "                   all          4         17       0.83      0.654      0.731      0.549\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     54/100      4.04G     0.3899     0.3489       1.07         19        640: 100% ━━━━━━━━━━━━ 1/1 7.5it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 28.9it/s 0.0s\n",
      "                   all          4         17      0.866      0.658       0.73       0.54\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     55/100      4.04G     0.3112     0.3031      0.946         38        640: 100% ━━━━━━━━━━━━ 1/1 8.9it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 20.7it/s 0.0s\n",
      "                   all          4         17      0.866      0.658       0.73       0.54\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     56/100      4.04G     0.3143     0.2849     0.9591         16        640: 100% ━━━━━━━━━━━━ 1/1 8.8it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 20.8it/s 0.0s\n",
      "                   all          4         17      0.866      0.658       0.73       0.54\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     57/100      4.04G     0.3795     0.3351     0.9224         26        640: 100% ━━━━━━━━━━━━ 1/1 7.2it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 27.2it/s 0.0s\n",
      "                   all          4         17      0.857      0.661      0.722      0.553\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     58/100      4.04G     0.4268     0.4275      1.036         19        640: 100% ━━━━━━━━━━━━ 1/1 8.8it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 20.5it/s 0.0s\n",
      "                   all          4         17      0.857      0.661      0.722      0.553\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     59/100      4.04G     0.4547     0.3521     0.9573         27        640: 100% ━━━━━━━━━━━━ 1/1 8.6it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 20.0it/s 0.0s\n",
      "                   all          4         17      0.857      0.661      0.722      0.553\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     60/100      4.04G     0.5371     0.4705      1.041         25        640: 100% ━━━━━━━━━━━━ 1/1 7.1it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 28.6it/s 0.0s\n",
      "                   all          4         17      0.843      0.665      0.722      0.559\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     61/100      4.04G     0.2771      0.317     0.8071         30        640: 100% ━━━━━━━━━━━━ 1/1 8.9it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 20.6it/s 0.0s\n",
      "                   all          4         17      0.843      0.665      0.722      0.559\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     62/100      4.04G     0.3378      0.262     0.9101         25        640: 100% ━━━━━━━━━━━━ 1/1 8.5it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 20.4it/s 0.0s\n",
      "                   all          4         17      0.843      0.665      0.722      0.559\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     63/100      4.04G      0.393     0.4041      1.042         26        640: 100% ━━━━━━━━━━━━ 1/1 6.8it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 27.7it/s 0.0s\n",
      "                   all          4         17       0.85      0.664      0.724      0.579\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     64/100      4.04G     0.3704     0.3064     0.9682         20        640: 100% ━━━━━━━━━━━━ 1/1 8.8it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 20.1it/s 0.0s\n",
      "                   all          4         17       0.85      0.664      0.724      0.579\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     65/100      4.04G      0.281     0.2949      0.923         19        640: 100% ━━━━━━━━━━━━ 1/1 8.7it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 20.4it/s 0.0s\n",
      "                   all          4         17       0.85      0.664      0.724      0.579\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     66/100      4.04G     0.4219     0.3429     0.9783         41        640: 100% ━━━━━━━━━━━━ 1/1 8.2it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 28.2it/s 0.0s\n",
      "                   all          4         17      0.849      0.665      0.725      0.579\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     67/100      4.04G     0.4355     0.3658      1.011         36        640: 100% ━━━━━━━━━━━━ 1/1 9.6it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 21.4it/s 0.0s\n",
      "                   all          4         17      0.849      0.665      0.725      0.579\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     68/100      4.04G     0.3776     0.3272     0.9164         39        640: 100% ━━━━━━━━━━━━ 1/1 9.4it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 21.8it/s 0.0s\n",
      "                   all          4         17      0.849      0.665      0.725      0.579\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     69/100      4.04G     0.4367     0.3247     0.9104         28        640: 100% ━━━━━━━━━━━━ 1/1 7.0it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 30.3it/s 0.0s\n",
      "                   all          4         17      0.671      0.665      0.726      0.562\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     70/100      4.04G     0.2833     0.3241     0.8927         19        640: 100% ━━━━━━━━━━━━ 1/1 4.9it/s 0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 20.0it/s 0.1s\n",
      "                   all          4         17      0.671      0.665      0.726      0.562\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     71/100      4.04G     0.3374     0.2639     0.9674         34        640: 100% ━━━━━━━━━━━━ 1/1 9.0it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 20.1it/s 0.0s\n",
      "                   all          4         17      0.671      0.665      0.726      0.562\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     72/100      4.04G     0.4376     0.3622      1.014         19        640: 100% ━━━━━━━━━━━━ 1/1 8.4it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 27.1it/s 0.0s\n",
      "                   all          4         17      0.839      0.665      0.725      0.581\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     73/100      4.04G      0.325     0.2773     0.8942         30        640: 100% ━━━━━━━━━━━━ 1/1 9.8it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 21.4it/s 0.0s\n",
      "                   all          4         17      0.839      0.665      0.725      0.581\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     74/100      4.04G     0.3673     0.2273      1.005         23        640: 100% ━━━━━━━━━━━━ 1/1 7.9it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 19.9it/s 0.1s\n",
      "                   all          4         17      0.839      0.665      0.725      0.581\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     75/100      4.04G     0.3913     0.3285     0.9312         22        640: 100% ━━━━━━━━━━━━ 1/1 7.8it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 29.2it/s 0.0s\n",
      "                   all          4         17      0.829      0.667      0.726      0.581\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     76/100      4.04G      0.376     0.2982     0.9837         31        640: 100% ━━━━━━━━━━━━ 1/1 8.9it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 19.2it/s 0.1s\n",
      "                   all          4         17      0.829      0.667      0.726      0.581\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     77/100      4.04G     0.3861     0.3748     0.9633         24        640: 100% ━━━━━━━━━━━━ 1/1 8.8it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 19.9it/s 0.1s\n",
      "                   all          4         17      0.829      0.667      0.726      0.581\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     78/100      4.04G     0.3971     0.3176     0.9965         34        640: 100% ━━━━━━━━━━━━ 1/1 7.0it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 26.8it/s 0.0s\n",
      "                   all          4         17       0.82      0.667      0.728      0.578\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     79/100      4.04G     0.4286     0.3856      1.041         34        640: 100% ━━━━━━━━━━━━ 1/1 8.9it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 20.3it/s 0.0s\n",
      "                   all          4         17       0.82      0.667      0.728      0.578\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     80/100      4.04G     0.3964     0.2773     0.9541         31        640: 100% ━━━━━━━━━━━━ 1/1 8.7it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 20.7it/s 0.0s\n",
      "                   all          4         17       0.82      0.667      0.728      0.578\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     81/100      4.04G     0.2736     0.3345     0.9525         13        640: 100% ━━━━━━━━━━━━ 1/1 7.3it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 26.0it/s 0.0s\n",
      "                   all          4         17      0.817      0.667       0.73      0.579\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     82/100      4.04G     0.4648     0.3483      1.206         19        640: 100% ━━━━━━━━━━━━ 1/1 9.1it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 19.5it/s 0.1s\n",
      "                   all          4         17      0.817      0.667       0.73      0.579\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     83/100      4.04G     0.4465     0.4471      1.167         18        640: 100% ━━━━━━━━━━━━ 1/1 8.3it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 20.3it/s 0.0s\n",
      "                   all          4         17      0.817      0.667       0.73      0.579\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     84/100      4.04G     0.3351     0.2581     0.9625         33        640: 100% ━━━━━━━━━━━━ 1/1 7.4it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 28.6it/s 0.0s\n",
      "                   all          4         17      0.798      0.669       0.73      0.579\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     85/100      4.04G      0.403      0.351      1.025         51        640: 100% ━━━━━━━━━━━━ 1/1 9.1it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 19.2it/s 0.1s\n",
      "                   all          4         17      0.798      0.669       0.73      0.579\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     86/100      4.04G     0.3491     0.3414     0.9345         47        640: 100% ━━━━━━━━━━━━ 1/1 8.8it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 19.7it/s 0.1s\n",
      "                   all          4         17      0.798      0.669       0.73      0.579\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     87/100      4.04G     0.6279     0.5141      1.214         22        640: 100% ━━━━━━━━━━━━ 1/1 8.8it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 19.6it/s 0.1s\n",
      "                   all          4         17      0.798      0.669       0.73      0.579\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     88/100      4.04G     0.2442     0.2534      0.873         38        640: 100% ━━━━━━━━━━━━ 1/1 7.4it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 30.0it/s 0.0s\n",
      "                   all          4         17      0.638      0.672      0.732       0.58\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     89/100      4.04G     0.2666     0.4643      1.015         15        640: 100% ━━━━━━━━━━━━ 1/1 9.1it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 20.2it/s 0.0s\n",
      "                   all          4         17      0.638      0.672      0.732       0.58\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     90/100      4.04G     0.4194      0.424     0.9665         21        640: 100% ━━━━━━━━━━━━ 1/1 8.7it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 20.5it/s 0.0s\n",
      "                   all          4         17      0.638      0.672      0.732       0.58\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     91/100      4.04G     0.3436     0.2343     0.9323         13        640: 100% ━━━━━━━━━━━━ 1/1 6.4it/s 0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 16.2it/s 0.1s\n",
      "                   all          4         17      0.638      0.672      0.732       0.58\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     92/100      4.04G     0.3005     0.2216     0.9389         13        640: 100% ━━━━━━━━━━━━ 1/1 7.1it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 11.3it/s 0.1s\n",
      "                   all          4         17       0.63      0.678      0.732      0.585\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     93/100      4.04G     0.2366     0.2249     0.8647         13        640: 100% ━━━━━━━━━━━━ 1/1 8.1it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 16.5it/s 0.1s\n",
      "                   all          4         17       0.63      0.678      0.732      0.585\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     94/100      4.04G     0.2011     0.2017     0.8033         13        640: 100% ━━━━━━━━━━━━ 1/1 9.6it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 21.9it/s 0.0s\n",
      "                   all          4         17       0.63      0.678      0.732      0.585\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     95/100      4.04G     0.3427     0.2771      0.934         13        640: 100% ━━━━━━━━━━━━ 1/1 9.2it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 17.0it/s 0.1s\n",
      "                   all          4         17       0.63      0.678      0.732      0.585\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     96/100      4.04G     0.2278     0.2164     0.8198         13        640: 100% ━━━━━━━━━━━━ 1/1 7.2it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 21.0it/s 0.0s\n",
      "                   all          4         17      0.624      0.679      0.732      0.584\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     97/100      4.04G     0.2424     0.2367     0.8463         13        640: 100% ━━━━━━━━━━━━ 1/1 8.8it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 16.1it/s 0.1s\n",
      "                   all          4         17      0.624      0.679      0.732      0.584\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     98/100      4.04G     0.2347     0.2077     0.8785         13        640: 100% ━━━━━━━━━━━━ 1/1 8.3it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 16.5it/s 0.1s\n",
      "                   all          4         17      0.624      0.679      0.732      0.584\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     99/100      4.04G     0.3276     0.2752     0.8468         13        640: 100% ━━━━━━━━━━━━ 1/1 8.9it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 16.8it/s 0.1s\n",
      "                   all          4         17      0.624      0.679      0.732      0.584\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K    100/100      4.04G     0.2742     0.2456     0.8678         13        640: 100% ━━━━━━━━━━━━ 1/1 6.4it/s 0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 16.6it/s 0.1s\n",
      "                   all          4         17      0.625      0.683      0.734      0.588\n",
      "\n",
      "100 epochs completed in 0.028 hours.\n",
      "Optimizer stripped from /workspace/AdaptVis/runs/detect/train/weights/last.pt, 136.9MB\n",
      "Optimizer stripped from /workspace/AdaptVis/runs/detect/train/weights/best.pt, 136.9MB\n",
      "\n",
      "Validating /workspace/AdaptVis/runs/detect/train/weights/best.pt...\n",
      "Ultralytics 8.3.230 🚀 Python-3.11.14 torch-2.4.1+cu121 CUDA:0 (NVIDIA GeForce RTX 4090, 48519MiB)\n",
      "Model summary (fused): 112 layers, 68,200,608 parameters, 0 gradients, 257.8 GFLOPs\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 31.1it/s 0.0s\n",
      "                   all          4         17      0.879      0.885      0.979      0.781\n",
      "                person          3         10          1      0.525      0.899      0.663\n",
      "                   dog          1          1      0.858          1      0.995      0.697\n",
      "                 horse          1          2      0.855          1      0.995      0.821\n",
      "              elephant          1          2          1      0.784      0.995      0.713\n",
      "              umbrella          1          1      0.767          1      0.995      0.995\n",
      "          potted plant          1          1      0.793          1      0.995      0.796\n",
      "Speed: 0.1ms preprocess, 4.7ms inference, 0.0ms loss, 0.6ms postprocess per image\n",
      "Results saved to \u001b[1m/workspace/AdaptVis/runs/detect/train\u001b[0m\n",
      "💡 Learn more at https://docs.ultralytics.com/modes/train\n"
     ]
    }
   ],
   "source": [
    "!yolo detect model=yolov8x.pt source='https://ultralytics.com/images/zidane.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8fed8ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50/412] processed\n",
      "[100/412] processed\n",
      "[150/412] processed\n",
      "[200/412] processed\n",
      "[250/412] processed\n",
      "[300/412] processed\n",
      "[350/412] processed\n",
      "[400/412] processed\n"
     ]
    }
   ],
   "source": [
    "!python build_contA_yolo_annotations.py \\\n",
    "  --raw_ann merged_cont_A.jsonl \\\n",
    "  --image_root ./ \\\n",
    "  --yolo_model yolov8x.pt \\\n",
    "  --out contA_with_yolo.jsonl \\\n",
    "  --conf 0.25 \\\n",
    "  --iou 0.45 \\\n",
    "  --max_dets 50 \\\n",
    "  --filter_by_question"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
