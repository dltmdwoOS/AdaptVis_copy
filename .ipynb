{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5bfd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash my_run.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd62f9b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "# from transformers import LlavaNextProcessor, LlavaNextForConditionalGeneration\n",
    "import random\n",
    "from transformers import AutoProcessor, LlamaTokenizerFast, CLIPImageProcessor\n",
    "import pdb\n",
    "# import probe_llava\n",
    "from model_zoo.llava import  LlavaForConditionalGeneration, LlavaForConditionalGenerationScal\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "from collections import Counter\n",
    "# from model_zoo.utils import normalize_answer,chat_completion_request,run_conversation\n",
    "\n",
    "from PIL import Image\n",
    "import math\n",
    "MODEL='llava-hf/llava-1.5-7b-hf'\n",
    "root_dir='data'\n",
    "device = 'cuda'\n",
    "#model = LlavaForConditionalGenerationScal.from_pretrained(MODEL, revision='a272c74',cache_dir=root_dir,ignore_mismatched_sizes=True).eval().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "188a76fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = CLIPImageProcessor.from_pretrained(MODEL, revision='a272c74',cache_dir=root_dir)\n",
    "tokenizer = LlamaTokenizerFast.from_pretrained(MODEL, revision='a272c74',cache_dir=root_dir)\n",
    "processor = AutoProcessor.from_pretrained(MODEL, revision='a272c74',cache_dir=root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55967e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''\\\n",
    "USER: Where is the violin in relation to the sofa? Think step by step, then answer about the relation between violin and sofa with left, right, on or under.\n",
    "ASSISTANT: In this picture, the brown violin is lying on the floor in the bottom-center area, while the beige fabric sofa stands prominently across the center-center region.\\\n",
    "Since the violin is positioned on the floor near the sofa's legs without making contact with the cushion, they are separated.\\\n",
    "From the camera's perspective, the violin appears to be in the foreground relative to the sofa backrest, suggesting a front-behind relationship.\\\n",
    "So, as the final answer to the question of where the violin is in relation to the sofa, the violin is in front of the sofa...Wait. I need to look closer at the vertical alignment. The violin is actually positioned directly beneath the seat cushion, obscured by the shadow. It is within the footprint of the sofa, not outside of it. Therefore, the correct relation is vertical, not horizontal.\\\n",
    "So, as the final answer to the question of where the violin is in relation to the sofa, the violin is under the sofa.\n",
    "USER: Where is the calculator in relation to the desk? Think step by step, then answer about the relation between calculator and desk with left, right, on or under.\n",
    "ASSISTANT: In this picture, the gray calculator is resting on the surface in the center-right section, and the wooden desk occupies the bottom-center to center-center area.\\\n",
    "Because the device is physically supported by the surface with no gap in between, they are touching.\\\n",
    "Since the calculator is placed flatly regarding gravity, the relative spatial relationship corresponds to the on-under category.\\\n",
    "So, as the final answer to the question of where the calculator is in relation to the desk, the calculator is on the floor... Wait. I made a mistake in identifying the reference object. The calculator is indeed lying flat, but it is supported by the desk, not the ground. The question asks for the relation to the desk.\\\n",
    "So, as the final answer to the question of where the calculator is in relation to the desk, the calculator is on the desk.\n",
    "USER: Where is the cat in relation to the rug? Think step by step, then answer about the relation between cat and rug with left, right, on or under.\n",
    "ASSISTANT: In this picture, the white cat is sitting upright in the bottom-right corner, while the patterned rug lies flat covering the bottom-center area.\\\n",
    "Since the cat is sitting on the floor adjacent to the rug's edge rather than on the fabric itself, they are separated.\\\n",
    "Because the two objects are side-by-side on the floor, the relative spatial relationship corresponds to the left-right category.\\\n",
    "So, as the final answer to the question of where the cat is in relation to the rug, the cat is next to the rug... Wait. The specific coordinates show the cat is in the bottom-right while the rug is center/bottom-center. \"Next to\" is too vague. The cat is clearly positioned to the eastern side of the rug.\\\n",
    "So, as the final answer to the question of where the cat is in relation to the rug, the cat is right of the rug.\n",
    "USER: Where is the stapler in relation to the printer? Think step by step, then answer about the relation between stapler and printer with left, right, on or under.\n",
    "ASSISTANT: In this picture, the blue stapler is sitting on the desk in the center-left region, while the large laser printer sits heavily in the center-right region.\\\n",
    "Since there is a clear span of empty desk surface between the two office supplies, they are separated.\\\n",
    "Because the stapler and printer are aligned horizontally on the desk, the relative spatial relationship corresponds to the left-right category.\\\n",
    "So, as the final answer to the question of where the stapler is in relation to the printer, the stapler is next to the printer... Wait. I need to be precise about the direction. The stapler is in the center-left region, and the printer is in the center-right. Therefore, the stapler is on the western side relative to the printer.\\\n",
    "So, as the final answer to the question of where the stapler is in relation to the printer, the stapler is left of the printer.\n",
    "'''\n",
    "t = tokenizer(text, return_tensors=\"pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58427d5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 982])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae808e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = [\"Left\", \"Right\", \"On\", \"Under\"]\n",
    "option_ids = [tokenizer.encode(r, add_special_tokens=False)[0] for r in options]\n",
    "option_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3c137b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "# from transformers import LlavaNextProcessor, LlavaNextForConditionalGeneration\n",
    "import random\n",
    "from transformers import AutoProcessor, LlamaTokenizerFast, CLIPImageProcessor\n",
    "import pdb\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "from collections import Counter\n",
    "# from model_zoo.utils import normalize_answer,chat_completion_request,run_conversation\n",
    "\n",
    "from PIL import Image\n",
    "import math\n",
    "MODEL='llava-hf/llava-1.5-7b-hf'\n",
    "\n",
    "import copy\n",
    "import inspect\n",
    "import warnings\n",
    "from dataclasses import dataclass\n",
    "from typing import TYPE_CHECKING, Any, Callable, Dict, List, Optional, Tuple, Union\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torch import nn\n",
    "\n",
    "from transformers.generation.logits_process import (\n",
    "    LogitsProcessorList,\n",
    ")\n",
    "from transformers.generation.stopping_criteria import (\n",
    "    StoppingCriteria,\n",
    "    StoppingCriteriaList,\n",
    "    validate_stopping_criteria,\n",
    ")\n",
    "import transformers\n",
    "from transformers.generation.utils import SampleOutput, SampleDecoderOnlyOutput, SampleEncoderDecoderOutput,GenerateEncoderDecoderOutput,GenerateDecoderOnlyOutput,GenerateNonBeamOutput\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "def _add_weight_greedy_search(\n",
    "    self,\n",
    "    input_ids: torch. LongTensor,\n",
    "    logits_processor: Optional[LogitsProcessorList] = None,\n",
    "    stopping_criteria: Optional[StoppingCriteriaList] = None,\n",
    "    max_length: Optional[int] = None,\n",
    "    pad_token_id: Optional[int] = None,\n",
    "    eos_token_id: Optional[Union[int, List[int]]] = None,\n",
    "    output_attentions: Optional[bool] = None,\n",
    "    output_hidden_states: Optional[bool] = None,\n",
    "    output_scores: Optional[bool] = None,\n",
    "    output_logits: Optional[bool] = None,\n",
    "    return_dict_in_generate: Optional[bool] = None,\n",
    "    synced_gpus: bool = False,\n",
    "    # keys:Optional[torch.Tensor] = None,\n",
    "    weight: Optional[float] = None,\n",
    "    adjust_method: Optional[str] = None,\n",
    "    pos: Optional[torch.Tensor] = None,\n",
    "    streamer: Optional[\"BaseStreamer\"] = None,\n",
    "    **model_kwargs,\n",
    "    ) -> Union[GenerateNonBeamOutput, torch.LongTensor]:\n",
    "    # init values\n",
    "    logits_processor = logits_processor if logits_processor is not None else LogitsProcessorList()\n",
    "    stopping_criteria = stopping_criteria if stopping_criteria is not None else StoppingCriteriaList()\n",
    "    if max_length is not None:\n",
    "        warnings.warn(\n",
    "            \"`max_length` is deprecated in this function, use\"\n",
    "            \" `stopping_criteria=StoppingCriteriaList([MaxLengthCriteria(max_length=max_length)])` instead.\",\n",
    "            UserWarning,\n",
    "        )\n",
    "        stopping_criteria = validate_stopping_criteria(stopping_criteria, max_length)\n",
    "    pad_token_id = pad_token_id if pad_token_id is not None else self.generation_config.pad_token_id\n",
    "    eos_token_id = eos_token_id if eos_token_id is not None else self.generation_config.eos_token_id\n",
    "    if isinstance(eos_token_id, int):\n",
    "        eos_token_id = [eos_token_id]\n",
    "    eos_token_id_tensor = torch.tensor(eos_token_id).to(input_ids.device) if eos_token_id is not None else None\n",
    "    output_scores = output_scores if output_scores is not None else self.generation_config.output_scores\n",
    "    output_attentions = (\n",
    "        output_attentions if output_attentions is not None else self.generation_config.output_attentions\n",
    "    )\n",
    "    output_hidden_states = (\n",
    "        output_hidden_states if output_hidden_states is not None else self.generation_config.output_hidden_states\n",
    "    )\n",
    "    return_dict_in_generate = (\n",
    "        return_dict_in_generate\n",
    "        if return_dict_in_generate is not None\n",
    "        else self.generation_config.return_dict_in_generate\n",
    "    )\n",
    "\n",
    "    # init attention / hidden states / scores tuples\n",
    "    raw_logits = () if (return_dict_in_generate and output_logits) else None\n",
    "    scores = () if (return_dict_in_generate and output_scores) else None\n",
    "    before = () if (return_dict_in_generate) else None\n",
    "    decoder_attentions = () if (return_dict_in_generate and output_attentions) else None\n",
    "    cross_attentions = () if (return_dict_in_generate and output_attentions) else None\n",
    "    decoder_hidden_states = () if (return_dict_in_generate and output_hidden_states) else None\n",
    "\n",
    "    # if model is an encoder-decoder, retrieve encoder attention weights and hidden states\n",
    "    if return_dict_in_generate and self.config.is_encoder_decoder:\n",
    "        encoder_attentions = model_kwargs[\"encoder_outputs\"].get(\"attentions\") if output_attentions else None\n",
    "        encoder_hidden_states = (\n",
    "            model_kwargs[\"encoder_outputs\"].get(\"hidden_states\") if output_hidden_states else None\n",
    "        )\n",
    "\n",
    "    # keep track of which sequences are already finished\n",
    "    batch_size, cur_len = input_ids.shape\n",
    "    if \"inputs_embeds\" in model_kwargs:\n",
    "        cur_len = model_kwargs[\"inputs_embeds\"].shape[1]\n",
    "    this_peer_finished = False\n",
    "    unfinished_sequences = torch.ones(batch_size, dtype=torch.long, device=input_ids.device)\n",
    "    model_kwargs[\"cache_position\"] = torch.arange(cur_len, device=input_ids.device)\n",
    "    \n",
    "    while self._has_unfinished_sequences(this_peer_finished, synced_gpus, device=input_ids.device):\n",
    "        # prepare model inputs\n",
    "        model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)\n",
    "        import pdb\n",
    "        # \n",
    "        if 'Scal' not in str(type(self)):\n",
    "            outputs = self(\n",
    "                **model_inputs,\n",
    "               \n",
    "                return_dict=True,\n",
    "                output_attentions=output_attentions,\n",
    "                output_hidden_states=output_hidden_states,\n",
    "            )\n",
    "        else:\n",
    "            \n",
    "            outputs = self(\n",
    "                **model_inputs,\n",
    "                weight=weight,\n",
    "                adjust_method=adjust_method,\n",
    "                pos=pos,\n",
    "                return_dict=True,\n",
    "                output_attentions=output_attentions,\n",
    "                output_hidden_states=output_hidden_states,\n",
    "            )\n",
    "\n",
    "        if synced_gpus and this_peer_finished:\n",
    "            continue  # don't waste resources running the code we don't need\n",
    "\n",
    "        next_token_logits = outputs.logits[:, -1, :]\n",
    "\n",
    "        # pre-process distribution\n",
    "        next_tokens_scores = logits_processor(input_ids, next_token_logits)\n",
    "\n",
    "        # Store scores, attentions and hidden_states when required\n",
    "        if return_dict_in_generate:\n",
    "            if output_scores:\n",
    "                scores += (next_tokens_scores,)\n",
    "            if output_logits:\n",
    "                raw_logits += (next_token_logits,)\n",
    "            if output_attentions:\n",
    "                decoder_attentions += (\n",
    "                    (outputs.decoder_attentions,) if self.config.is_encoder_decoder else (outputs.attentions,)\n",
    "                )\n",
    "                if self.config.is_encoder_decoder:\n",
    "                    cross_attentions += (outputs.cross_attentions,)\n",
    "\n",
    "            if output_hidden_states:\n",
    "                decoder_hidden_states += (\n",
    "                    (outputs.decoder_hidden_states,)\n",
    "                    if self.config.is_encoder_decoder\n",
    "                    else (outputs.hidden_states,)\n",
    "                )\n",
    "\n",
    "        # argmax\n",
    "        next_tokens = torch.argmax(next_tokens_scores, dim=-1)\n",
    "\n",
    "        # finished sentences should have their next token be a padding token\n",
    "        if eos_token_id is not None:\n",
    "            if pad_token_id is None:\n",
    "                raise ValueError(\"If `eos_token_id` is defined, make sure that `pad_token_id` is defined.\")\n",
    "            next_tokens = next_tokens * unfinished_sequences + pad_token_id * (1 - unfinished_sequences)\n",
    "\n",
    "        # update generated ids, model inputs, and length for next step\n",
    "        input_ids = torch.cat([input_ids, next_tokens[:, None]], dim=-1)\n",
    "        if streamer is not None:\n",
    "            streamer.put(next_tokens.cpu())\n",
    "        model_kwargs = self._update_model_kwargs_for_generation(\n",
    "            outputs,\n",
    "            model_kwargs,\n",
    "            is_encoder_decoder=self.config.is_encoder_decoder,\n",
    "        )\n",
    "\n",
    "        # if eos_token was found in one sentence, set sentence to finished\n",
    "        if eos_token_id_tensor is not None:\n",
    "            unfinished_sequences = unfinished_sequences.mul(\n",
    "                next_tokens.tile(eos_token_id_tensor.shape[0], 1).ne(eos_token_id_tensor.unsqueeze(1)).prod(dim=0)\n",
    "            )\n",
    "\n",
    "        unfinished_sequences = unfinished_sequences & ~stopping_criteria(input_ids, scores)\n",
    "        this_peer_finished = unfinished_sequences.max() == 0\n",
    "\n",
    "    if streamer is not None:\n",
    "        streamer.end()\n",
    "\n",
    "    if return_dict_in_generate:\n",
    "        if self.config.is_encoder_decoder:\n",
    "            return GenerateEncoderDecoderOutput(\n",
    "                sequences=input_ids,\n",
    "                scores=scores,\n",
    "                logits=raw_logits,\n",
    "                encoder_attentions=encoder_attentions,\n",
    "                encoder_hidden_states=encoder_hidden_states,\n",
    "                decoder_attentions=decoder_attentions,\n",
    "                cross_attentions=cross_attentions,\n",
    "                decoder_hidden_states=decoder_hidden_states,\n",
    "                past_key_values=model_kwargs.get(\"past_key_values\"),\n",
    "            )\n",
    "        else:\n",
    "            return GenerateDecoderOnlyOutput(\n",
    "                sequences=input_ids,\n",
    "                scores=scores,\n",
    "                logits=raw_logits,\n",
    "                attentions=decoder_attentions,\n",
    "                hidden_states=decoder_hidden_states,\n",
    "                past_key_values=model_kwargs.get(\"past_key_values\"),\n",
    "            )\n",
    "    else:\n",
    "        return input_ids\n",
    "    \n",
    "def change_greedy_to_add_weight():\n",
    "    transformers.generation.utils.GenerationMixin._greedy_search = _add_weight_greedy_search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d0dfb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Hello, my name is \"\n",
    "prompt = f\"USER: {query}\\nASSISTANT:\"\n",
    "single_input = processor.tokenizer(\n",
    "    text=prompt,\n",
    "    padding=\"max_length\", \n",
    "    return_tensors=\"pt\", \n",
    "    max_length=77\n",
    ").to(device)\n",
    "\n",
    "keys = [torch.where(input_id == 32001, 1, 0) for input_id in single_input['input_ids']]\n",
    "change_greedy_to_add_weight()\n",
    "\n",
    "output = model.generate(\n",
    "    **single_input,\n",
    "    keys=keys,\n",
    "    weight=1.0,\n",
    "    max_new_tokens=100, \n",
    "    output_scores=True, \n",
    "    return_dict_in_generate=True\n",
    ")\n",
    "real_output = output['sequences'][0][len(single_input['input_ids'][-1]):]\n",
    "gens = [processor.decode(token, skip_special_tokens=True) for token in real_output]\n",
    "gen = processor.decode(real_output, skip_special_tokens=True).strip()\n",
    "probs = [torch.log_softmax(s[0], dim=-1).max().item() for s in output['scores']]\n",
    "confidence = float(np.mean(probs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f56244",
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash my_run.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6426621",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "prompts_path = 'prompts/Controlled_Images_A_with_answer_four_options.jsonl'\n",
    "image_paths_path = 'data/controlled_images_dataset.json'\n",
    "\n",
    "prompts = []\n",
    "image_paths = None\n",
    "\n",
    "with open(prompts_path) as f:\n",
    "    for line in f:\n",
    "        prompts.append(json.loads(line))\n",
    "with open(image_paths_path) as f:\n",
    "    image_paths = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68610e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def merge_lists(list1, list2):\n",
    "    merged = []\n",
    "    pattern = re.compile(r\"<image>\\nUSER:(.*)\\nASSISTANT:\", re.DOTALL)\n",
    "\n",
    "    for d1, d2 in zip(list1, list2):\n",
    "        q = d1[\"question\"]\n",
    "        m = pattern.search(q)\n",
    "        if not m:\n",
    "            raise ValueError(f\"Question format unexpected: {q!r}\")\n",
    "        question_text = m.group(1).strip()\n",
    "\n",
    "        merged.append(\n",
    "            {\n",
    "                \"id\": d1[\"id\"],\n",
    "                \"image_path\": d2[\"image_path\"],\n",
    "                \"question\": question_text,\n",
    "                \"answer_label\": d1[\"answer\"][0],\n",
    "            }\n",
    "        )\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1adbbe61",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = merge_lists(prompts, image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18353c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"merged_cont_A.jsonl\", 'w') as f:\n",
    "    for data in merged:\n",
    "        f.write(json.dumps(data) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef89088",
   "metadata": {},
   "outputs": [],
   "source": [
    "!yolo detect model=yolov8x.pt source='https://ultralytics.com/images/zidane.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fed8ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python build_contA_yolo_annotations.py \\\n",
    "  --raw_ann merged_cont_A.jsonl \\\n",
    "  --image_root ./ \\\n",
    "  --yolo_model yolov8x.pt \\\n",
    "  --out contA_with_yolo.jsonl \\\n",
    "  --conf 0.25 \\\n",
    "  --iou 0.45 \\\n",
    "  --max_dets 50 \\\n",
    "  --filter_by_question"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
