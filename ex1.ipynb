{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "791b73c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/venv/env1/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████████████| 3/3 [00:48<00:00, 16.29s/it]\n",
      "Some weights of LlavaForConditionalGenerationScal were not initialized from the model checkpoint at llava-hf/llava-1.5-7b-hf and are newly initialized: ['language_model.model.layers.0.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.1.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.10.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.11.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.12.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.13.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.14.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.15.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.16.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.17.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.18.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.19.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.2.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.20.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.21.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.22.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.23.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.24.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.25.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.26.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.27.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.28.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.29.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.3.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.30.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.31.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.4.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.5.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.6.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.7.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.8.self_attn.rotary_emb.inv_freq', 'language_model.model.layers.9.self_attn.rotary_emb.inv_freq']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Controlled_Images_A llava1.5\n",
      "  0%|                                                   | 0/330 [00:00<?, ?it/s]\n",
      "Probabilities of options: tensor([0.3284, 0.2214, 0.1855, 0.2647], device='cuda:0')\n",
      "\n",
      "Uncertainty_prob: 0.3210330009460449  |  Uncertainty_KL: 0.0056005362421274185  |  Threshold: 0.4\n",
      "Prompt: <image>\n",
      "USER: Where is the beer bottle in relation to the armchair? Answer with left, right, on or under.\n",
      "ASSISTANT:\n",
      "Generation: The beer bottle is on the armchair.\n",
      "Golden: On\n",
      "1 1 1.0\n",
      "  0%|                                         | 1/330 [00:21<1:58:30, 21.61s/it]\n",
      "Probabilities of options: tensor([0.1113, 0.6402, 0.0525, 0.1960], device='cuda:0')\n",
      "\n",
      "Uncertainty_prob: 0.6276255249977112  |  Uncertainty_KL: 0.09458544850349426  |  Threshold: 0.4\n",
      "Prompt: <image>\n",
      "USER: Where is the scarf in relation to the armchair? Answer with left, right, on or under.\n",
      "ASSISTANT:\n",
      "Generation: Right\n",
      "Golden: Right\n",
      "2 2 1.0\n",
      "  1%|▏                                        | 2/330 [00:32<1:23:33, 15.28s/it]\n",
      "Probabilities of options: tensor([0.1823, 0.5734, 0.0452, 0.1990], device='cuda:0')\n",
      "\n",
      "Uncertainty_prob: 0.5658017992973328  |  Uncertainty_KL: 0.07586197555065155  |  Threshold: 0.4\n",
      "Prompt: <image>\n",
      "USER: Where is the orange in relation to the armchair? Answer with left, right, on or under.\n",
      "ASSISTANT:\n",
      "Generation: Right\n",
      "Golden: Right\n",
      "3 3 1.0\n",
      "  1%|▎                                        | 3/330 [00:43<1:12:29, 13.30s/it]\n",
      "Probabilities of options: tensor([0.1414, 0.3174, 0.1333, 0.4080], device='cuda:0')\n",
      "\n",
      "Uncertainty_prob: 0.401756227016449  |  Uncertainty_KL: 0.028237247839570045  |  Threshold: 0.4\n",
      "Prompt: <image>\n",
      "USER: Where is the scarf in relation to the table? Answer with left, right, on or under.\n",
      "ASSISTANT:\n",
      "Generation: Under\n",
      "Golden: Under\n",
      "4 4 1.0\n",
      "  1%|▍                                        | 4/330 [00:53<1:05:48, 12.11s/it]\n",
      "Probabilities of options: tensor([0.7676, 0.0983, 0.0227, 0.1114], device='cuda:0')\n",
      "\n",
      "Uncertainty_prob: 0.7613427042961121  |  Uncertainty_KL: 0.15517660975456238  |  Threshold: 0.4\n",
      "Prompt: <image>\n",
      "USER: Where are the sunglasses in relation to the chair? Answer with left, right, on or under.\n",
      "ASSISTANT:\n",
      "Generation: Left\n",
      "Golden: Left\n",
      "5 5 1.0\n",
      "  2%|▌                                        | 5/330 [01:03<1:01:41, 11.39s/it]\n",
      "Probabilities of options: tensor([0.5912, 0.1906, 0.0444, 0.1739], device='cuda:0')\n",
      "\n",
      "Uncertainty_prob: 0.5789944529533386  |  Uncertainty_KL: 0.0806562528014183  |  Threshold: 0.4\n",
      "Prompt: <image>\n",
      "USER: Where is the toilet roll in relation to the table? Answer with left, right, on or under.\n",
      "ASSISTANT:\n",
      "Generation: Left\n",
      "Golden: Left\n",
      "6 6 1.0\n",
      "  2%|▊                                          | 6/330 [01:13<59:18, 10.98s/it]\n",
      "Probabilities of options: tensor([0.1265, 0.5932, 0.0723, 0.2080], device='cuda:0')\n",
      "\n",
      "Uncertainty_prob: 0.5863107442855835  |  Uncertainty_KL: 0.07315850257873535  |  Threshold: 0.4\n",
      "Prompt: <image>\n",
      "USER: Where is the bottle in relation to the chair? Answer with left, right, on or under.\n",
      "ASSISTANT:\n",
      "Generation: Right\n",
      "Golden: Right\n",
      "7 7 1.0\n",
      "  2%|▉                                          | 7/330 [01:24<58:30, 10.87s/it]\n",
      "Probabilities of options: tensor([0.1474, 0.5949, 0.0718, 0.1859], device='cuda:0')\n",
      "\n",
      "Uncertainty_prob: 0.5883249640464783  |  Uncertainty_KL: 0.07141728699207306  |  Threshold: 0.4\n",
      "Prompt: <image>\n",
      "USER: Where is the cup in relation to the chair? Answer with left, right, on or under.\n",
      "ASSISTANT:\n",
      "Generation: Right\n",
      "Golden: Right\n",
      "8 8 1.0\n",
      "  2%|█                                          | 8/330 [01:34<56:26, 10.52s/it]\n",
      "Probabilities of options: tensor([0.6939, 0.1049, 0.0241, 0.1771], device='cuda:0')\n",
      "\n",
      "Uncertainty_prob: 0.6881883144378662  |  Uncertainty_KL: 0.12687897682189941  |  Threshold: 0.4\n",
      "Prompt: <image>\n",
      "USER: Where is the bowl in relation to the armchair? Answer with left, right, on or under.\n",
      "ASSISTANT:\n",
      "Generation: Left\n",
      "Golden: Left\n",
      "9 9 1.0\n",
      "  3%|█▏                                         | 9/330 [01:44<55:26, 10.36s/it]\n",
      "Probabilities of options: tensor([0.1457, 0.5569, 0.0682, 0.2292], device='cuda:0')\n",
      "\n",
      "Uncertainty_prob: 0.5519288182258606  |  Uncertainty_KL: 0.06468351185321808  |  Threshold: 0.4\n",
      "Prompt: <image>\n",
      "USER: Where is the mug in relation to the table? Answer with left, right, on or under.\n",
      "ASSISTANT:\n",
      "Generation: Right\n",
      "Golden: Right\n",
      "10 10 1.0\n",
      "  3%|█▎                                        | 10/330 [01:54<54:47, 10.27s/it]\n",
      "Probabilities of options: tensor([0.4225, 0.1931, 0.1670, 0.2175], device='cuda:0')\n",
      "\n",
      "Uncertainty_prob: 0.4167553186416626  |  Uncertainty_KL: 0.017740346491336823  |  Threshold: 0.4\n",
      "Prompt: <image>\n",
      "USER: Where is the mug in relation to the table? Answer with left, right, on or under.\n",
      "ASSISTANT:\n",
      "Generation: Left\n",
      "Golden: On\n",
      "10 11 0.9090909090909091\n",
      "  3%|█▍                                        | 11/330 [02:04<53:39, 10.09s/it]\n",
      "Probabilities of options: tensor([0.4230, 0.2373, 0.1404, 0.1993], device='cuda:0')\n",
      "\n",
      "Uncertainty_prob: 0.4166768193244934  |  Uncertainty_KL: 0.020564623177051544  |  Threshold: 0.4\n",
      "Prompt: <image>\n",
      "USER: Where is the cup in relation to the armchair? Answer with left, right, on or under.\n",
      "ASSISTANT:\n",
      "Generation: Left\n",
      "Golden: On\n",
      "10 12 0.8333333333333334\n",
      "  4%|█▌                                        | 12/330 [02:13<53:01, 10.00s/it]\n",
      "Probabilities of options: tensor([0.8548, 0.0745, 0.0207, 0.0500], device='cuda:0')\n",
      "\n",
      "Uncertainty_prob: 0.8483000993728638  |  Uncertainty_KL: 0.2062382698059082  |  Threshold: 0.4\n",
      "Prompt: <image>\n",
      "USER: Where are the shoes in relation to the chair? Answer with left, right, on or under.\n",
      "ASSISTANT:\n",
      "Generation: Left\n",
      "Golden: Left\n",
      "11 13 0.8461538461538461\n",
      "  4%|█▋                                        | 13/330 [02:23<52:31,  9.94s/it]\n",
      "Probabilities of options: tensor([0.1815, 0.5286, 0.0742, 0.2158], device='cuda:0')\n",
      "\n",
      "Uncertainty_prob: 0.5206400156021118  |  Uncertainty_KL: 0.054021477699279785  |  Threshold: 0.4\n",
      "Prompt: <image>\n",
      "USER: Where is the orange in relation to the chair? Answer with left, right, on or under.\n",
      "ASSISTANT:\n",
      "Generation: Right\n",
      "Golden: Right\n",
      "12 14 0.8571428571428571\n",
      "  4%|█▊                                        | 14/330 [02:33<51:49,  9.84s/it]\n",
      "Probabilities of options: tensor([0.1329, 0.5829, 0.0587, 0.2255], device='cuda:0')\n",
      "\n",
      "Uncertainty_prob: 0.5771519541740417  |  Uncertainty_KL: 0.07554727792739868  |  Threshold: 0.4\n",
      "Prompt: <image>\n",
      "USER: Where is the helmet in relation to the table? Answer with left, right, on or under.\n",
      "ASSISTANT:\n",
      "Generation: Right\n",
      "Golden: Right\n",
      "13 15 0.8666666666666667\n",
      "  5%|█▉                                        | 15/330 [02:43<51:56,  9.89s/it]\n",
      "Probabilities of options: tensor([0.1568, 0.6139, 0.0505, 0.1789], device='cuda:0')\n",
      "\n",
      "Uncertainty_prob: 0.6060140132904053  |  Uncertainty_KL: 0.0839991420507431  |  Threshold: 0.4\n",
      "Prompt: <image>\n",
      "USER: Where is the water filter in relation to the chair? Answer with left, right, on or under.\n",
      "ASSISTANT:\n",
      "Generation: Right\n",
      "Golden: Right\n",
      "14 16 0.875\n",
      "  5%|██                                        | 16/330 [02:53<51:55,  9.92s/it]\n",
      "Probabilities of options: tensor([0.6706, 0.1614, 0.0208, 0.1473], device='cuda:0')\n",
      "\n",
      "Uncertainty_prob: 0.6647372245788574  |  Uncertainty_KL: 0.11859376728534698  |  Threshold: 0.4\n",
      "Prompt: <image>\n",
      "USER: Where is the glove in relation to the armchair? Answer with left, right, on or under.\n",
      "ASSISTANT:\n",
      "Generation: Left\n",
      "Golden: Left\n",
      "15 17 0.8823529411764706\n",
      "  5%|██▏                                       | 17/330 [03:03<52:24, 10.05s/it]\n",
      "Probabilities of options: tensor([0.6196, 0.1472, 0.0789, 0.1543], device='cuda:0')\n",
      "\n",
      "Uncertainty_prob: 0.611416757106781  |  Uncertainty_KL: 0.07638701051473618  |  Threshold: 0.4\n",
      "Prompt: <image>\n",
      "USER: Where is the dragonfruit in relation to the table? Answer with left, right, on or under.\n",
      "ASSISTANT:\n",
      "Generation: Left\n",
      "Golden: Left\n",
      "16 18 0.8888888888888888\n",
      "  5%|██▎                                       | 18/330 [03:16<55:49, 10.74s/it]\n",
      "Probabilities of options: tensor([0.0762, 0.6204, 0.0745, 0.2289], device='cuda:0')\n",
      "\n",
      "Uncertainty_prob: 0.6139964461326599  |  Uncertainty_KL: 0.09034693241119385  |  Threshold: 0.4\n",
      "Prompt: <image>\n",
      "USER: Where is the scarf in relation to the table? Answer with left, right, on or under.\n",
      "ASSISTANT:\n",
      "Generation: Right\n",
      "Golden: Right\n",
      "17 19 0.8947368421052632\n",
      "  6%|██▍                                       | 19/330 [03:26<55:27, 10.70s/it]\n",
      "Probabilities of options: tensor([0.3455, 0.2690, 0.0886, 0.2968], device='cuda:0')\n",
      "\n",
      "Uncertainty_prob: 0.33868029713630676  |  Uncertainty_KL: 0.025061694905161858  |  Threshold: 0.4\n",
      "Prompt: <image>\n",
      "USER: Where is the water filter in relation to the chair? Answer with left, right, on or under.\n",
      "ASSISTANT:\n",
      "Generation: The water filter is on the chair.\n",
      "Golden: On\n",
      "18 20 0.9\n",
      "  6%|██▌                                       | 20/330 [03:39<58:18, 11.29s/it]^C\n",
      "Process Process-15:\n",
      "Process Process-14:\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7b0b0a137130>\n",
      "Traceback (most recent call last):\n",
      "  File \"/venv/env1/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1477, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/venv/env1/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1441, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/venv/env1/lib/python3.10/multiprocessing/process.py\", line 149, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "  File \"/venv/env1/lib/python3.10/multiprocessing/popen_fork.py\", line 40, in wait\n",
      "    if not wait([self.sentinel], timeout):\n",
      "  File \"/venv/env1/lib/python3.10/multiprocessing/connection.py\", line 931, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/venv/env1/lib/python3.10/selectors.py\", line 416, in select\n",
      "    fd_event_list = self._selector.poll(timeout)\n",
      "KeyboardInterrupt: \n",
      "  6%|██▌                                       | 20/330 [03:42<57:28, 11.12s/it]\n",
      "Traceback (most recent call last):\n",
      "  File \"/workspace/AdaptVis_copy/main_aro.py\", line 93, in <module>\n",
      "    main(args)\n",
      "  File \"/workspace/AdaptVis_copy/main_aro.py\", line 78, in main\n",
      "    scores, correct_id = model.get_out_scores_wh_batched(args.dataset,joint_loader,args.method,args.weight,args.option,args.threshold,args.weight1,args.weight2)\n",
      "  File \"/venv/env1/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/workspace/AdaptVis_copy/model_zoo/llava15.py\", line 417, in get_out_scores_wh_batched\n",
      "    output = self.model.generate(\n",
      "  File \"/venv/env1/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/venv/env1/lib/python3.10/site-packages/transformers/generation/utils.py\", line 1527, in generate\n",
      "    result = self._greedy_search(\n",
      "  File \"/workspace/AdaptVis_copy/model_zoo/llava15.py\", line 121, in _add_weight_greedy_search\n",
      "    outputs = self(\n",
      "  File \"/venv/env1/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/venv/env1/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/workspace/AdaptVis_copy/model_zoo/llava/modeling_llava_scal.py\", line 458, in forward\n",
      "    outputs = self.language_model(\n",
      "  File \"/venv/env1/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/venv/env1/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/workspace/AdaptVis_copy/model_zoo/llama/modeling_llama_add_attn.py\", line 872, in forward\n",
      "    outputs = self.model(\n",
      "  File \"/venv/env1/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/venv/env1/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/workspace/AdaptVis_copy/model_zoo/llama/modeling_llama_add_attn.py\", line 708, in forward\n",
      "    layer_outputs = decoder_layer(\n",
      "  File \"/venv/env1/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/venv/env1/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/workspace/AdaptVis_copy/model_zoo/llama/modeling_llama_add_attn.py\", line 389, in forward\n",
      "    hidden_states, self_attn_weights, present_key_value = self.self_attn(\n",
      "  File \"/venv/env1/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/venv/env1/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/workspace/AdaptVis_copy/model_zoo/llama/modeling_llama_add_attn.py\", line 329, in forward\n",
      "    attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!bash ex1.sh"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
